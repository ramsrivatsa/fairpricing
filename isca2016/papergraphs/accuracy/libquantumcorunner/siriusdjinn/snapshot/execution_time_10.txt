colocating with libquantum
gmm
********* Setting up benchmark gmm
********* Running benchmark as ./gmm_scoring
338.31user 2.40system 9:47.11elapsed 58%CPU (0avgtext+0avgdata 171664maxresident)k
0inputs+288outputs (0major+10806minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
gmm
********* Setting up benchmark gmm
********* Running benchmark as ./gmm_scoring
342.38user 2.12system 9:55.48elapsed 57%CPU (0avgtext+0avgdata 171648maxresident)k
0inputs+288outputs (0major+10804minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
gmm
********* Setting up benchmark gmm
********* Running benchmark as ./gmm_scoring
354.93user 2.20system 10:16.05elapsed 57%CPU (0avgtext+0avgdata 171664maxresident)k
0inputs+288outputs (0major+10805minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
gmm
********* Setting up benchmark gmm
********* Running benchmark as ./gmm_scoring
353.26user 2.20system 10:11.69elapsed 58%CPU (0avgtext+0avgdata 171648maxresident)k
0inputs+288outputs (0major+10804minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
gmm
********* Setting up benchmark gmm
********* Running benchmark as ./gmm_scoring
352.50user 2.17system 10:10.24elapsed 58%CPU (0avgtext+0avgdata 171664maxresident)k
0inputs+288outputs (0major+10804minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
dnn_asr
********* Setting up benchmark dnn-asr
********* Running benchmark as ./dnn_asr
769.06user 5.72system 22:13.98elapsed 58%CPU (0avgtext+0avgdata 1738928maxresident)k
419056inputs+8outputs (54major+174747minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
dnn_asr
********* Setting up benchmark dnn-asr
********* Running benchmark as ./dnn_asr
791.65user 5.56system 22:55.96elapsed 57%CPU (0avgtext+0avgdata 1738784maxresident)k
0inputs+8outputs (0major+174795minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
dnn_asr
********* Setting up benchmark dnn-asr
********* Running benchmark as ./dnn_asr
790.00user 5.59system 22:56.00elapsed 57%CPU (0avgtext+0avgdata 1738944maxresident)k
0inputs+8outputs (0major+174802minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
dnn_asr
********* Setting up benchmark dnn-asr
********* Running benchmark as ./dnn_asr
801.02user 5.27system 23:11.72elapsed 57%CPU (0avgtext+0avgdata 1738928maxresident)k
0inputs+8outputs (0major+174802minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
dnn_asr
********* Setting up benchmark dnn-asr
********* Running benchmark as ./dnn_asr
794.90user 5.81system 22:55.73elapsed 58%CPU (0avgtext+0avgdata 1738896maxresident)k
0inputs+8outputs (0major+174800minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fe
********* Setting up benchmark fe
********* Running benchmark as ./surf-fe
297.57user 1.64system 8:36.81elapsed 57%CPU (0avgtext+0avgdata 52976maxresident)k
42904inputs+8outputs (151major+3338minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fe
********* Setting up benchmark fe
********* Running benchmark as ./surf-fe
298.88user 1.61system 8:40.77elapsed 57%CPU (0avgtext+0avgdata 52992maxresident)k
0inputs+8outputs (0major+3490minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fe
********* Setting up benchmark fe
********* Running benchmark as ./surf-fe
297.39user 2.08system 8:37.44elapsed 57%CPU (0avgtext+0avgdata 52992maxresident)k
0inputs+8outputs (0major+3490minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fe
********* Setting up benchmark fe
********* Running benchmark as ./surf-fe
296.94user 2.02system 8:34.38elapsed 58%CPU (0avgtext+0avgdata 52992maxresident)k
0inputs+8outputs (0major+3490minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fe
********* Setting up benchmark fe
********* Running benchmark as ./surf-fe
297.51user 1.94system 8:35.87elapsed 58%CPU (0avgtext+0avgdata 52992maxresident)k
0inputs+8outputs (0major+3490minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fd
********* Setting up benchmark fd
********* Running benchmark as ./surf-fd
342.88user 2.22system 9:57.18elapsed 57%CPU (0avgtext+0avgdata 52960maxresident)k
2752inputs+8outputs (1major+3493minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fd
********* Setting up benchmark fd
********* Running benchmark as ./surf-fd
343.63user 2.26system 9:57.61elapsed 57%CPU (0avgtext+0avgdata 52960maxresident)k
0inputs+8outputs (0major+3494minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fd
********* Setting up benchmark fd
********* Running benchmark as ./surf-fd
343.02user 2.07system 9:54.41elapsed 58%CPU (0avgtext+0avgdata 52944maxresident)k
0inputs+8outputs (0major+3493minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fd
********* Setting up benchmark fd
********* Running benchmark as ./surf-fd
343.96user 1.84system 9:56.36elapsed 57%CPU (0avgtext+0avgdata 52960maxresident)k
0inputs+8outputs (0major+3494minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
surf-fd
********* Setting up benchmark fd
********* Running benchmark as ./surf-fd
344.53user 1.63system 9:57.42elapsed 57%CPU (0avgtext+0avgdata 52944maxresident)k
0inputs+8outputs (0major+3493minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
stem
********* Setting up benchmark stemmer
********* Running benchmark as ./stem_porter
725.35user 3.08system 20:58.99elapsed 57%CPU (0avgtext+0avgdata 21632maxresident)k
568inputs+8outputs (1major+1389minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
stem
********* Setting up benchmark stemmer
********* Running benchmark as ./stem_porter
722.45user 3.72system 20:52.62elapsed 57%CPU (0avgtext+0avgdata 21632maxresident)k
0inputs+8outputs (0major+1389minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
stem
********* Setting up benchmark stemmer
********* Running benchmark as ./stem_porter
717.65user 3.47system 20:44.11elapsed 57%CPU (0avgtext+0avgdata 21648maxresident)k
0inputs+8outputs (0major+1390minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
stem
********* Setting up benchmark stemmer
********* Running benchmark as ./stem_porter
717.01user 3.54system 20:40.80elapsed 58%CPU (0avgtext+0avgdata 21648maxresident)k
0inputs+8outputs (0major+1390minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
stem
********* Setting up benchmark stemmer
********* Running benchmark as ./stem_porter
716.43user 3.49system 20:41.70elapsed 57%CPU (0avgtext+0avgdata 21648maxresident)k
0inputs+8outputs (0major+1390minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
regex
********* Setting up benchmark regex
********* Running benchmark as ./regex_slre
406.51user 2.04system 11:47.16elapsed 57%CPU (0avgtext+0avgdata 14096maxresident)k
104inputs+8outputs (1major+926minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
regex
********* Setting up benchmark regex
********* Running benchmark as ./regex_slre
416.33user 2.17system 12:02.90elapsed 57%CPU (0avgtext+0avgdata 14096maxresident)k
0inputs+8outputs (0major+927minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
regex
********* Setting up benchmark regex
********* Running benchmark as ./regex_slre
404.16user 2.03system 11:39.93elapsed 58%CPU (0avgtext+0avgdata 14096maxresident)k
0inputs+8outputs (0major+927minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
regex
********* Setting up benchmark regex
********* Running benchmark as ./regex_slre
413.15user 2.20system 11:54.13elapsed 58%CPU (0avgtext+0avgdata 14112maxresident)k
0inputs+8outputs (0major+928minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
regex
********* Setting up benchmark regex
********* Running benchmark as ./regex_slre
409.98user 2.15system 11:49.77elapsed 58%CPU (0avgtext+0avgdata 14096maxresident)k
0inputs+8outputs (0major+927minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
crf
********* Setting up benchmark crf
********* Running benchmark as ./crf_tag
1185.52user 6.97system 34:25.23elapsed 57%CPU (0avgtext+0avgdata 387520maxresident)k
40432inputs+8outputs (1major+28829minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
crf
********* Setting up benchmark crf
********* Running benchmark as ./crf_tag
1190.92user 6.70system 34:20.00elapsed 58%CPU (0avgtext+0avgdata 387520maxresident)k
0inputs+8outputs (0major+28830minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
crf
********* Setting up benchmark crf
********* Running benchmark as ./crf_tag
1154.77user 6.25system 33:10.51elapsed 58%CPU (0avgtext+0avgdata 387520maxresident)k
0inputs+8outputs (0major+28830minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
crf
********* Setting up benchmark crf
********* Running benchmark as ./crf_tag
1168.60user 6.24system 33:35.97elapsed 58%CPU (0avgtext+0avgdata 387520maxresident)k
0inputs+8outputs (0major+28830minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
crf
********* Setting up benchmark crf
********* Running benchmark as ./crf_tag
1173.20user 6.53system 33:40.06elapsed 58%CPU (0avgtext+0avgdata 387520maxresident)k
0inputs+8outputs (0major+28831minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-imc
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-imc
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 09:00:13.886718 22364 net.cpp:39] Initializing net from parameters: 
name: "imc"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 1000
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227
I0802 09:00:13.888391 22364 net.cpp:358] Input 0 -> data
I0802 09:00:13.888743 22364 net.cpp:67] Creating Layer conv1
I0802 09:00:13.888815 22364 net.cpp:394] conv1 <- data
I0802 09:00:13.888876 22364 net.cpp:356] conv1 -> conv1
I0802 09:00:13.889021 22364 net.cpp:96] Setting up conv1
I0802 09:00:13.892894 22364 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:00:13.894507 22364 net.cpp:67] Creating Layer relu1
I0802 09:00:13.894618 22364 net.cpp:394] relu1 <- conv1
I0802 09:00:13.894690 22364 net.cpp:345] relu1 -> conv1 (in-place)
I0802 09:00:13.894922 22364 net.cpp:96] Setting up relu1
I0802 09:00:13.894999 22364 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:00:13.895068 22364 net.cpp:67] Creating Layer pool1
I0802 09:00:13.895128 22364 net.cpp:394] pool1 <- conv1
I0802 09:00:13.895184 22364 net.cpp:356] pool1 -> pool1
I0802 09:00:13.895274 22364 net.cpp:96] Setting up pool1
I0802 09:00:13.896850 22364 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:00:13.897245 22364 net.cpp:67] Creating Layer norm1
I0802 09:00:13.897305 22364 net.cpp:394] norm1 <- pool1
I0802 09:00:13.897470 22364 net.cpp:356] norm1 -> norm1
I0802 09:00:13.897579 22364 net.cpp:96] Setting up norm1
I0802 09:00:13.897652 22364 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:00:13.897780 22364 net.cpp:67] Creating Layer conv2
I0802 09:00:13.897830 22364 net.cpp:394] conv2 <- norm1
I0802 09:00:13.897883 22364 net.cpp:356] conv2 -> conv2
I0802 09:00:13.897953 22364 net.cpp:96] Setting up conv2
I0802 09:00:13.899514 22364 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:00:13.899988 22364 net.cpp:67] Creating Layer relu2
I0802 09:00:13.900043 22364 net.cpp:394] relu2 <- conv2
I0802 09:00:13.900189 22364 net.cpp:345] relu2 -> conv2 (in-place)
I0802 09:00:13.900275 22364 net.cpp:96] Setting up relu2
I0802 09:00:13.900332 22364 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:00:13.900390 22364 net.cpp:67] Creating Layer pool2
I0802 09:00:13.900444 22364 net.cpp:394] pool2 <- conv2
I0802 09:00:13.900493 22364 net.cpp:356] pool2 -> pool2
I0802 09:00:13.900563 22364 net.cpp:96] Setting up pool2
I0802 09:00:13.900625 22364 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:00:13.900713 22364 net.cpp:67] Creating Layer norm2
I0802 09:00:13.900758 22364 net.cpp:394] norm2 <- pool2
I0802 09:00:13.900820 22364 net.cpp:356] norm2 -> norm2
I0802 09:00:13.900887 22364 net.cpp:96] Setting up norm2
I0802 09:00:13.900954 22364 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:00:13.901031 22364 net.cpp:67] Creating Layer conv3
I0802 09:00:13.901087 22364 net.cpp:394] conv3 <- norm2
I0802 09:00:13.901149 22364 net.cpp:356] conv3 -> conv3
I0802 09:00:13.901217 22364 net.cpp:96] Setting up conv3
I0802 09:00:13.906929 22364 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:00:13.907563 22364 net.cpp:67] Creating Layer relu3
I0802 09:00:13.907692 22364 net.cpp:394] relu3 <- conv3
I0802 09:00:13.907773 22364 net.cpp:345] relu3 -> conv3 (in-place)
I0802 09:00:13.907951 22364 net.cpp:96] Setting up relu3
I0802 09:00:13.908002 22364 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:00:13.908074 22364 net.cpp:67] Creating Layer conv4
I0802 09:00:13.908116 22364 net.cpp:394] conv4 <- conv3
I0802 09:00:13.908179 22364 net.cpp:356] conv4 -> conv4
I0802 09:00:13.908251 22364 net.cpp:96] Setting up conv4
I0802 09:00:13.912629 22364 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:00:13.913326 22364 net.cpp:67] Creating Layer relu4
I0802 09:00:13.913396 22364 net.cpp:394] relu4 <- conv4
I0802 09:00:13.913578 22364 net.cpp:345] relu4 -> conv4 (in-place)
I0802 09:00:13.913668 22364 net.cpp:96] Setting up relu4
I0802 09:00:13.913717 22364 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:00:13.913807 22364 net.cpp:67] Creating Layer conv5
I0802 09:00:13.913849 22364 net.cpp:394] conv5 <- conv4
I0802 09:00:13.913915 22364 net.cpp:356] conv5 -> conv5
I0802 09:00:13.913974 22364 net.cpp:96] Setting up conv5
I0802 09:00:13.916211 22364 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:00:13.916908 22364 net.cpp:67] Creating Layer relu5
I0802 09:00:13.916993 22364 net.cpp:394] relu5 <- conv5
I0802 09:00:13.917152 22364 net.cpp:345] relu5 -> conv5 (in-place)
I0802 09:00:13.917268 22364 net.cpp:96] Setting up relu5
I0802 09:00:13.917315 22364 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:00:13.917378 22364 net.cpp:67] Creating Layer pool5
I0802 09:00:13.917454 22364 net.cpp:394] pool5 <- conv5
I0802 09:00:13.917512 22364 net.cpp:356] pool5 -> pool5
I0802 09:00:13.917596 22364 net.cpp:96] Setting up pool5
I0802 09:00:13.917672 22364 net.cpp:103] Top shape: 1 256 6 6 (9216)
I0802 09:00:13.917757 22364 net.cpp:67] Creating Layer fc6
I0802 09:00:13.917804 22364 net.cpp:394] fc6 <- pool5
I0802 09:00:13.917875 22364 net.cpp:356] fc6 -> fc6
I0802 09:00:13.917992 22364 net.cpp:96] Setting up fc6
I0802 09:00:14.661207 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.662027 22364 net.cpp:67] Creating Layer relu6
I0802 09:00:14.662099 22364 net.cpp:394] relu6 <- fc6
I0802 09:00:14.662163 22364 net.cpp:345] relu6 -> fc6 (in-place)
I0802 09:00:14.662295 22364 net.cpp:96] Setting up relu6
I0802 09:00:14.662363 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.662428 22364 net.cpp:67] Creating Layer drop6
I0802 09:00:14.662475 22364 net.cpp:394] drop6 <- fc6
I0802 09:00:14.662545 22364 net.cpp:345] drop6 -> fc6 (in-place)
I0802 09:00:14.662650 22364 net.cpp:96] Setting up drop6
I0802 09:00:14.662757 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.662822 22364 net.cpp:67] Creating Layer fc7
I0802 09:00:14.662868 22364 net.cpp:394] fc7 <- fc6
I0802 09:00:14.662966 22364 net.cpp:356] fc7 -> fc7
I0802 09:00:14.663041 22364 net.cpp:96] Setting up fc7
I0802 09:00:14.944000 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.944689 22364 net.cpp:67] Creating Layer relu7
I0802 09:00:14.944775 22364 net.cpp:394] relu7 <- fc7
I0802 09:00:14.944855 22364 net.cpp:345] relu7 -> fc7 (in-place)
I0802 09:00:14.944974 22364 net.cpp:96] Setting up relu7
I0802 09:00:14.945039 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.945082 22364 net.cpp:67] Creating Layer drop7
I0802 09:00:14.945127 22364 net.cpp:394] drop7 <- fc7
I0802 09:00:14.945212 22364 net.cpp:345] drop7 -> fc7 (in-place)
I0802 09:00:14.945273 22364 net.cpp:96] Setting up drop7
I0802 09:00:14.945350 22364 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:00:14.945422 22364 net.cpp:67] Creating Layer fc8
I0802 09:00:14.945475 22364 net.cpp:394] fc8 <- fc7
I0802 09:00:14.945549 22364 net.cpp:356] fc8 -> fc8
I0802 09:00:14.945647 22364 net.cpp:96] Setting up fc8
I0802 09:00:15.011013 22364 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:00:15.011895 22364 net.cpp:67] Creating Layer prob
I0802 09:00:15.011970 22364 net.cpp:394] prob <- fc8
I0802 09:00:15.012048 22364 net.cpp:356] prob -> prob
I0802 09:00:15.012166 22364 net.cpp:96] Setting up prob
I0802 09:00:15.012362 22364 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:00:15.012483 22364 net.cpp:67] Creating Layer argmax
I0802 09:00:15.012538 22364 net.cpp:394] argmax <- prob
I0802 09:00:15.012608 22364 net.cpp:356] argmax -> argmax
I0802 09:00:15.012672 22364 net.cpp:96] Setting up argmax
I0802 09:00:15.013649 22364 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 09:00:15.013785 22364 net.cpp:172] argmax does not need backward computation.
I0802 09:00:15.013838 22364 net.cpp:172] prob does not need backward computation.
I0802 09:00:15.013916 22364 net.cpp:172] fc8 does not need backward computation.
I0802 09:00:15.013969 22364 net.cpp:172] drop7 does not need backward computation.
I0802 09:00:15.014019 22364 net.cpp:172] relu7 does not need backward computation.
I0802 09:00:15.014067 22364 net.cpp:172] fc7 does not need backward computation.
I0802 09:00:15.014119 22364 net.cpp:172] drop6 does not need backward computation.
I0802 09:00:15.014173 22364 net.cpp:172] relu6 does not need backward computation.
I0802 09:00:15.014221 22364 net.cpp:172] fc6 does not need backward computation.
I0802 09:00:15.014271 22364 net.cpp:172] pool5 does not need backward computation.
I0802 09:00:15.014322 22364 net.cpp:172] relu5 does not need backward computation.
I0802 09:00:15.014372 22364 net.cpp:172] conv5 does not need backward computation.
I0802 09:00:15.014422 22364 net.cpp:172] relu4 does not need backward computation.
I0802 09:00:15.014474 22364 net.cpp:172] conv4 does not need backward computation.
I0802 09:00:15.014529 22364 net.cpp:172] relu3 does not need backward computation.
I0802 09:00:15.014582 22364 net.cpp:172] conv3 does not need backward computation.
I0802 09:00:15.014632 22364 net.cpp:172] norm2 does not need backward computation.
I0802 09:00:15.014686 22364 net.cpp:172] pool2 does not need backward computation.
I0802 09:00:15.014741 22364 net.cpp:172] relu2 does not need backward computation.
I0802 09:00:15.014796 22364 net.cpp:172] conv2 does not need backward computation.
I0802 09:00:15.014853 22364 net.cpp:172] norm1 does not need backward computation.
I0802 09:00:15.014905 22364 net.cpp:172] pool1 does not need backward computation.
I0802 09:00:15.014979 22364 net.cpp:172] relu1 does not need backward computation.
I0802 09:00:15.015041 22364 net.cpp:172] conv1 does not need backward computation.
I0802 09:00:15.015100 22364 net.cpp:208] This network produces output argmax
I0802 09:00:15.015307 22364 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 09:00:15.016331 22364 net.cpp:219] Network initialization done.
I0802 09:00:15.016470 22364 net.cpp:220] Memory required for data: 6249796
E0802 09:00:18.791332 22364 upgrade_proto.cpp:615] Attempting to upgrade input file specified using deprecated transformation parameters: ../../common/weights/imc.caffemodel
I0802 09:00:18.792135 22364 upgrade_proto.cpp:618] Successfully upgraded file specified using deprecated data transformation parameters.
E0802 09:00:18.792281 22364 upgrade_proto.cpp:620] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0802 09:00:19.295334 22364 img-client.cpp:139] Reading input/imc/dog-208-227.jpg
I0802 09:00:19.306864 22364 img-client.cpp:139] Reading input/imc/cat-281-227.jpg
I0802 09:00:19.317559 22364 img-client.cpp:139] Reading input/imc/tp-999-227.jpg
I0802 09:00:19.330569 22364 tonic.cpp:31] Reshaping input to dims: 3 3 227 227
I0802 09:00:19.337700 22364 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 09:18:25.311101 22364 img-client.cpp:222] Image: input/imc/dog-208-227.jpg class: 208
I0802 09:18:25.317298 22364 img-client.cpp:222] Image: input/imc/cat-281-227.jpg class: 281
I0802 09:18:25.317420 22364 img-client.cpp:222] Image: input/imc/tp-999-227.jpg class: 999
631.00user 5.30system 18:11.72elapsed 58%CPU (0avgtext+0avgdata 2216512maxresident)k
484832inputs+8outputs (27major+217345minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-imc
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-imc
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 09:18:36.663275  9543 net.cpp:39] Initializing net from parameters: 
name: "imc"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 1000
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227
I0802 09:18:36.669342  9543 net.cpp:358] Input 0 -> data
I0802 09:18:36.671527  9543 net.cpp:67] Creating Layer conv1
I0802 09:18:36.672003  9543 net.cpp:394] conv1 <- data
I0802 09:18:36.672155  9543 net.cpp:356] conv1 -> conv1
I0802 09:18:36.672327  9543 net.cpp:96] Setting up conv1
I0802 09:18:36.672893  9543 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:18:36.673441  9543 net.cpp:67] Creating Layer relu1
I0802 09:18:36.673498  9543 net.cpp:394] relu1 <- conv1
I0802 09:18:36.673581  9543 net.cpp:345] relu1 -> conv1 (in-place)
I0802 09:18:36.673645  9543 net.cpp:96] Setting up relu1
I0802 09:18:36.673743  9543 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:18:36.673823  9543 net.cpp:67] Creating Layer pool1
I0802 09:18:36.673871  9543 net.cpp:394] pool1 <- conv1
I0802 09:18:36.673923  9543 net.cpp:356] pool1 -> pool1
I0802 09:18:36.674005  9543 net.cpp:96] Setting up pool1
I0802 09:18:36.674129  9543 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:18:36.674199  9543 net.cpp:67] Creating Layer norm1
I0802 09:18:36.674245  9543 net.cpp:394] norm1 <- pool1
I0802 09:18:36.674304  9543 net.cpp:356] norm1 -> norm1
I0802 09:18:36.674370  9543 net.cpp:96] Setting up norm1
I0802 09:18:36.674437  9543 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:18:36.674504  9543 net.cpp:67] Creating Layer conv2
I0802 09:18:36.674556  9543 net.cpp:394] conv2 <- norm1
I0802 09:18:36.674607  9543 net.cpp:356] conv2 -> conv2
I0802 09:18:36.674671  9543 net.cpp:96] Setting up conv2
I0802 09:18:36.676367  9543 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:18:36.676851  9543 net.cpp:67] Creating Layer relu2
I0802 09:18:36.676913  9543 net.cpp:394] relu2 <- conv2
I0802 09:18:36.677083  9543 net.cpp:345] relu2 -> conv2 (in-place)
I0802 09:18:36.677176  9543 net.cpp:96] Setting up relu2
I0802 09:18:36.677247  9543 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:18:36.677302  9543 net.cpp:67] Creating Layer pool2
I0802 09:18:36.677356  9543 net.cpp:394] pool2 <- conv2
I0802 09:18:36.677407  9543 net.cpp:356] pool2 -> pool2
I0802 09:18:36.677470  9543 net.cpp:96] Setting up pool2
I0802 09:18:36.677541  9543 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:18:36.677623  9543 net.cpp:67] Creating Layer norm2
I0802 09:18:36.677696  9543 net.cpp:394] norm2 <- pool2
I0802 09:18:36.677752  9543 net.cpp:356] norm2 -> norm2
I0802 09:18:36.677819  9543 net.cpp:96] Setting up norm2
I0802 09:18:36.677913  9543 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:18:36.677979  9543 net.cpp:67] Creating Layer conv3
I0802 09:18:36.678043  9543 net.cpp:394] conv3 <- norm2
I0802 09:18:36.678093  9543 net.cpp:356] conv3 -> conv3
I0802 09:18:36.678165  9543 net.cpp:96] Setting up conv3
I0802 09:18:36.685458  9543 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:18:36.686213  9543 net.cpp:67] Creating Layer relu3
I0802 09:18:36.686275  9543 net.cpp:394] relu3 <- conv3
I0802 09:18:36.686523  9543 net.cpp:345] relu3 -> conv3 (in-place)
I0802 09:18:36.686630  9543 net.cpp:96] Setting up relu3
I0802 09:18:36.686687  9543 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:18:36.686760  9543 net.cpp:67] Creating Layer conv4
I0802 09:18:36.686810  9543 net.cpp:394] conv4 <- conv3
I0802 09:18:36.686861  9543 net.cpp:356] conv4 -> conv4
I0802 09:18:36.686949  9543 net.cpp:96] Setting up conv4
I0802 09:18:36.693052  9543 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:18:36.693756  9543 net.cpp:67] Creating Layer relu4
I0802 09:18:36.693827  9543 net.cpp:394] relu4 <- conv4
I0802 09:18:36.694041  9543 net.cpp:345] relu4 -> conv4 (in-place)
I0802 09:18:36.694139  9543 net.cpp:96] Setting up relu4
I0802 09:18:36.694195  9543 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:18:36.694243  9543 net.cpp:67] Creating Layer conv5
I0802 09:18:36.694280  9543 net.cpp:394] conv5 <- conv4
I0802 09:18:36.694344  9543 net.cpp:356] conv5 -> conv5
I0802 09:18:36.694404  9543 net.cpp:96] Setting up conv5
I0802 09:18:36.696782  9543 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:18:36.697640  9543 net.cpp:67] Creating Layer relu5
I0802 09:18:36.697738  9543 net.cpp:394] relu5 <- conv5
I0802 09:18:36.697865  9543 net.cpp:345] relu5 -> conv5 (in-place)
I0802 09:18:36.697986  9543 net.cpp:96] Setting up relu5
I0802 09:18:36.698040  9543 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:18:36.698122  9543 net.cpp:67] Creating Layer pool5
I0802 09:18:36.698169  9543 net.cpp:394] pool5 <- conv5
I0802 09:18:36.698220  9543 net.cpp:356] pool5 -> pool5
I0802 09:18:36.698295  9543 net.cpp:96] Setting up pool5
I0802 09:18:36.698357  9543 net.cpp:103] Top shape: 1 256 6 6 (9216)
I0802 09:18:36.698421  9543 net.cpp:67] Creating Layer fc6
I0802 09:18:36.698467  9543 net.cpp:394] fc6 <- pool5
I0802 09:18:36.698536  9543 net.cpp:356] fc6 -> fc6
I0802 09:18:36.698637  9543 net.cpp:96] Setting up fc6
I0802 09:18:37.253700  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.254446  9543 net.cpp:67] Creating Layer relu6
I0802 09:18:37.254516  9543 net.cpp:394] relu6 <- fc6
I0802 09:18:37.254585  9543 net.cpp:345] relu6 -> fc6 (in-place)
I0802 09:18:37.254716  9543 net.cpp:96] Setting up relu6
I0802 09:18:37.254775  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.254830  9543 net.cpp:67] Creating Layer drop6
I0802 09:18:37.254899  9543 net.cpp:394] drop6 <- fc6
I0802 09:18:37.254952  9543 net.cpp:345] drop6 -> fc6 (in-place)
I0802 09:18:37.255065  9543 net.cpp:96] Setting up drop6
I0802 09:18:37.255175  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.255247  9543 net.cpp:67] Creating Layer fc7
I0802 09:18:37.255292  9543 net.cpp:394] fc7 <- fc6
I0802 09:18:37.255372  9543 net.cpp:356] fc7 -> fc7
I0802 09:18:37.255445  9543 net.cpp:96] Setting up fc7
I0802 09:18:37.457350  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.458209  9543 net.cpp:67] Creating Layer relu7
I0802 09:18:37.458282  9543 net.cpp:394] relu7 <- fc7
I0802 09:18:37.458366  9543 net.cpp:345] relu7 -> fc7 (in-place)
I0802 09:18:37.458483  9543 net.cpp:96] Setting up relu7
I0802 09:18:37.458536  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.458616  9543 net.cpp:67] Creating Layer drop7
I0802 09:18:37.458662  9543 net.cpp:394] drop7 <- fc7
I0802 09:18:37.458730  9543 net.cpp:345] drop7 -> fc7 (in-place)
I0802 09:18:37.458794  9543 net.cpp:96] Setting up drop7
I0802 09:18:37.458849  9543 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:18:37.458921  9543 net.cpp:67] Creating Layer fc8
I0802 09:18:37.458964  9543 net.cpp:394] fc8 <- fc7
I0802 09:18:37.459025  9543 net.cpp:356] fc8 -> fc8
I0802 09:18:37.459105  9543 net.cpp:96] Setting up fc8
I0802 09:18:37.521049  9543 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:18:37.524453  9543 net.cpp:67] Creating Layer prob
I0802 09:18:37.525563  9543 net.cpp:394] prob <- fc8
I0802 09:18:37.525709  9543 net.cpp:356] prob -> prob
I0802 09:18:37.525836  9543 net.cpp:96] Setting up prob
I0802 09:18:37.526010  9543 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:18:37.526388  9543 net.cpp:67] Creating Layer argmax
I0802 09:18:37.526449  9543 net.cpp:394] argmax <- prob
I0802 09:18:37.526512  9543 net.cpp:356] argmax -> argmax
I0802 09:18:37.526583  9543 net.cpp:96] Setting up argmax
I0802 09:18:37.526666  9543 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 09:18:37.526716  9543 net.cpp:172] argmax does not need backward computation.
I0802 09:18:37.529386  9543 net.cpp:172] prob does not need backward computation.
I0802 09:18:37.529800  9543 net.cpp:172] fc8 does not need backward computation.
I0802 09:18:37.529944  9543 net.cpp:172] drop7 does not need backward computation.
I0802 09:18:37.529989  9543 net.cpp:172] relu7 does not need backward computation.
I0802 09:18:37.530050  9543 net.cpp:172] fc7 does not need backward computation.
I0802 09:18:37.530102  9543 net.cpp:172] drop6 does not need backward computation.
I0802 09:18:37.530155  9543 net.cpp:172] relu6 does not need backward computation.
I0802 09:18:37.530206  9543 net.cpp:172] fc6 does not need backward computation.
I0802 09:18:37.530257  9543 net.cpp:172] pool5 does not need backward computation.
I0802 09:18:37.530309  9543 net.cpp:172] relu5 does not need backward computation.
I0802 09:18:37.530361  9543 net.cpp:172] conv5 does not need backward computation.
I0802 09:18:37.530411  9543 net.cpp:172] relu4 does not need backward computation.
I0802 09:18:37.530462  9543 net.cpp:172] conv4 does not need backward computation.
I0802 09:18:37.530513  9543 net.cpp:172] relu3 does not need backward computation.
I0802 09:18:37.530565  9543 net.cpp:172] conv3 does not need backward computation.
I0802 09:18:37.530617  9543 net.cpp:172] norm2 does not need backward computation.
I0802 09:18:37.530670  9543 net.cpp:172] pool2 does not need backward computation.
I0802 09:18:37.530724  9543 net.cpp:172] relu2 does not need backward computation.
I0802 09:18:37.530777  9543 net.cpp:172] conv2 does not need backward computation.
I0802 09:18:37.530834  9543 net.cpp:172] norm1 does not need backward computation.
I0802 09:18:37.530894  9543 net.cpp:172] pool1 does not need backward computation.
I0802 09:18:37.530968  9543 net.cpp:172] relu1 does not need backward computation.
I0802 09:18:37.531031  9543 net.cpp:172] conv1 does not need backward computation.
I0802 09:18:37.531095  9543 net.cpp:208] This network produces output argmax
I0802 09:18:37.531371  9543 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 09:18:37.531545  9543 net.cpp:219] Network initialization done.
I0802 09:18:37.531787  9543 net.cpp:220] Memory required for data: 6249796
E0802 09:18:39.774293  9543 upgrade_proto.cpp:615] Attempting to upgrade input file specified using deprecated transformation parameters: ../../common/weights/imc.caffemodel
I0802 09:18:39.775090  9543 upgrade_proto.cpp:618] Successfully upgraded file specified using deprecated data transformation parameters.
E0802 09:18:39.775148  9543 upgrade_proto.cpp:620] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0802 09:18:40.182590  9543 img-client.cpp:139] Reading input/imc/dog-208-227.jpg
I0802 09:18:40.193143  9543 img-client.cpp:139] Reading input/imc/cat-281-227.jpg
I0802 09:18:40.205966  9543 img-client.cpp:139] Reading input/imc/tp-999-227.jpg
I0802 09:18:40.216476  9543 tonic.cpp:31] Reshaping input to dims: 3 3 227 227
I0802 09:18:40.219492  9543 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 09:36:58.510918  9543 img-client.cpp:222] Image: input/imc/dog-208-227.jpg class: 208
I0802 09:36:58.513795  9543 img-client.cpp:222] Image: input/imc/cat-281-227.jpg class: 281
I0802 09:36:58.513934  9543 img-client.cpp:222] Image: input/imc/tp-999-227.jpg class: 999
640.50user 4.41system 18:22.18elapsed 58%CPU (0avgtext+0avgdata 2216480maxresident)k
0inputs+8outputs (0major+217371minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-imc
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-imc
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 09:37:09.998602 30954 net.cpp:39] Initializing net from parameters: 
name: "imc"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 1000
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227
I0802 09:37:10.002727 30954 net.cpp:358] Input 0 -> data
I0802 09:37:10.003396 30954 net.cpp:67] Creating Layer conv1
I0802 09:37:10.003495 30954 net.cpp:394] conv1 <- data
I0802 09:37:10.003864 30954 net.cpp:356] conv1 -> conv1
I0802 09:37:10.004045 30954 net.cpp:96] Setting up conv1
I0802 09:37:10.004668 30954 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:37:10.004940 30954 net.cpp:67] Creating Layer relu1
I0802 09:37:10.004993 30954 net.cpp:394] relu1 <- conv1
I0802 09:37:10.005081 30954 net.cpp:345] relu1 -> conv1 (in-place)
I0802 09:37:10.005148 30954 net.cpp:96] Setting up relu1
I0802 09:37:10.005228 30954 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:37:10.005307 30954 net.cpp:67] Creating Layer pool1
I0802 09:37:10.005360 30954 net.cpp:394] pool1 <- conv1
I0802 09:37:10.005441 30954 net.cpp:356] pool1 -> pool1
I0802 09:37:10.005533 30954 net.cpp:96] Setting up pool1
I0802 09:37:10.005699 30954 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:37:10.005784 30954 net.cpp:67] Creating Layer norm1
I0802 09:37:10.005831 30954 net.cpp:394] norm1 <- pool1
I0802 09:37:10.005899 30954 net.cpp:356] norm1 -> norm1
I0802 09:37:10.005981 30954 net.cpp:96] Setting up norm1
I0802 09:37:10.006063 30954 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:37:10.006134 30954 net.cpp:67] Creating Layer conv2
I0802 09:37:10.006204 30954 net.cpp:394] conv2 <- norm1
I0802 09:37:10.006268 30954 net.cpp:356] conv2 -> conv2
I0802 09:37:10.006348 30954 net.cpp:96] Setting up conv2
I0802 09:37:10.008303 30954 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:37:10.008940 30954 net.cpp:67] Creating Layer relu2
I0802 09:37:10.009006 30954 net.cpp:394] relu2 <- conv2
I0802 09:37:10.009210 30954 net.cpp:345] relu2 -> conv2 (in-place)
I0802 09:37:10.009336 30954 net.cpp:96] Setting up relu2
I0802 09:37:10.009418 30954 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:37:10.009490 30954 net.cpp:67] Creating Layer pool2
I0802 09:37:10.009564 30954 net.cpp:394] pool2 <- conv2
I0802 09:37:10.009632 30954 net.cpp:356] pool2 -> pool2
I0802 09:37:10.009798 30954 net.cpp:96] Setting up pool2
I0802 09:37:10.009891 30954 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:37:10.009979 30954 net.cpp:67] Creating Layer norm2
I0802 09:37:10.010037 30954 net.cpp:394] norm2 <- pool2
I0802 09:37:10.010087 30954 net.cpp:356] norm2 -> norm2
I0802 09:37:10.010169 30954 net.cpp:96] Setting up norm2
I0802 09:37:10.010247 30954 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:37:10.010336 30954 net.cpp:67] Creating Layer conv3
I0802 09:37:10.010381 30954 net.cpp:394] conv3 <- norm2
I0802 09:37:10.010426 30954 net.cpp:356] conv3 -> conv3
I0802 09:37:10.010510 30954 net.cpp:96] Setting up conv3
I0802 09:37:10.017379 30954 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:37:10.018076 30954 net.cpp:67] Creating Layer relu3
I0802 09:37:10.018131 30954 net.cpp:394] relu3 <- conv3
I0802 09:37:10.018313 30954 net.cpp:345] relu3 -> conv3 (in-place)
I0802 09:37:10.018393 30954 net.cpp:96] Setting up relu3
I0802 09:37:10.018451 30954 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:37:10.018504 30954 net.cpp:67] Creating Layer conv4
I0802 09:37:10.018556 30954 net.cpp:394] conv4 <- conv3
I0802 09:37:10.018606 30954 net.cpp:356] conv4 -> conv4
I0802 09:37:10.018682 30954 net.cpp:96] Setting up conv4
I0802 09:37:10.023208 30954 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:37:10.023908 30954 net.cpp:67] Creating Layer relu4
I0802 09:37:10.023972 30954 net.cpp:394] relu4 <- conv4
I0802 09:37:10.024171 30954 net.cpp:345] relu4 -> conv4 (in-place)
I0802 09:37:10.024262 30954 net.cpp:96] Setting up relu4
I0802 09:37:10.024337 30954 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:37:10.024420 30954 net.cpp:67] Creating Layer conv5
I0802 09:37:10.024466 30954 net.cpp:394] conv5 <- conv4
I0802 09:37:10.024525 30954 net.cpp:356] conv5 -> conv5
I0802 09:37:10.024616 30954 net.cpp:96] Setting up conv5
I0802 09:37:10.027223 30954 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:37:10.027959 30954 net.cpp:67] Creating Layer relu5
I0802 09:37:10.028029 30954 net.cpp:394] relu5 <- conv5
I0802 09:37:10.028256 30954 net.cpp:345] relu5 -> conv5 (in-place)
I0802 09:37:10.028355 30954 net.cpp:96] Setting up relu5
I0802 09:37:10.028421 30954 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:37:10.028477 30954 net.cpp:67] Creating Layer pool5
I0802 09:37:10.028530 30954 net.cpp:394] pool5 <- conv5
I0802 09:37:10.028589 30954 net.cpp:356] pool5 -> pool5
I0802 09:37:10.028656 30954 net.cpp:96] Setting up pool5
I0802 09:37:10.028724 30954 net.cpp:103] Top shape: 1 256 6 6 (9216)
I0802 09:37:10.028800 30954 net.cpp:67] Creating Layer fc6
I0802 09:37:10.028854 30954 net.cpp:394] fc6 <- pool5
I0802 09:37:10.028919 30954 net.cpp:356] fc6 -> fc6
I0802 09:37:10.029031 30954 net.cpp:96] Setting up fc6
I0802 09:37:10.540194 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.540776 30954 net.cpp:67] Creating Layer relu6
I0802 09:37:10.540848 30954 net.cpp:394] relu6 <- fc6
I0802 09:37:10.540927 30954 net.cpp:345] relu6 -> fc6 (in-place)
I0802 09:37:10.541031 30954 net.cpp:96] Setting up relu6
I0802 09:37:10.541097 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.541168 30954 net.cpp:67] Creating Layer drop6
I0802 09:37:10.541218 30954 net.cpp:394] drop6 <- fc6
I0802 09:37:10.541299 30954 net.cpp:345] drop6 -> fc6 (in-place)
I0802 09:37:10.541396 30954 net.cpp:96] Setting up drop6
I0802 09:37:10.541522 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.541610 30954 net.cpp:67] Creating Layer fc7
I0802 09:37:10.541659 30954 net.cpp:394] fc7 <- fc6
I0802 09:37:10.541745 30954 net.cpp:356] fc7 -> fc7
I0802 09:37:10.541821 30954 net.cpp:96] Setting up fc7
I0802 09:37:10.734259 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.735052 30954 net.cpp:67] Creating Layer relu7
I0802 09:37:10.735126 30954 net.cpp:394] relu7 <- fc7
I0802 09:37:10.735239 30954 net.cpp:345] relu7 -> fc7 (in-place)
I0802 09:37:10.735350 30954 net.cpp:96] Setting up relu7
I0802 09:37:10.735406 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.735478 30954 net.cpp:67] Creating Layer drop7
I0802 09:37:10.735534 30954 net.cpp:394] drop7 <- fc7
I0802 09:37:10.735677 30954 net.cpp:345] drop7 -> fc7 (in-place)
I0802 09:37:10.735736 30954 net.cpp:96] Setting up drop7
I0802 09:37:10.735798 30954 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:37:10.735949 30954 net.cpp:67] Creating Layer fc8
I0802 09:37:10.736006 30954 net.cpp:394] fc8 <- fc7
I0802 09:37:10.736078 30954 net.cpp:356] fc8 -> fc8
I0802 09:37:10.736173 30954 net.cpp:96] Setting up fc8
I0802 09:37:10.777324 30954 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:37:10.784915 30954 net.cpp:67] Creating Layer prob
I0802 09:37:10.785363 30954 net.cpp:394] prob <- fc8
I0802 09:37:10.785557 30954 net.cpp:356] prob -> prob
I0802 09:37:10.785689 30954 net.cpp:96] Setting up prob
I0802 09:37:10.785835 30954 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:37:10.785936 30954 net.cpp:67] Creating Layer argmax
I0802 09:37:10.785985 30954 net.cpp:394] argmax <- prob
I0802 09:37:10.786056 30954 net.cpp:356] argmax -> argmax
I0802 09:37:10.786140 30954 net.cpp:96] Setting up argmax
I0802 09:37:10.786195 30954 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 09:37:10.786253 30954 net.cpp:172] argmax does not need backward computation.
I0802 09:37:10.786352 30954 net.cpp:172] prob does not need backward computation.
I0802 09:37:10.786408 30954 net.cpp:172] fc8 does not need backward computation.
I0802 09:37:10.786466 30954 net.cpp:172] drop7 does not need backward computation.
I0802 09:37:10.786526 30954 net.cpp:172] relu7 does not need backward computation.
I0802 09:37:10.786594 30954 net.cpp:172] fc7 does not need backward computation.
I0802 09:37:10.786687 30954 net.cpp:172] drop6 does not need backward computation.
I0802 09:37:10.786743 30954 net.cpp:172] relu6 does not need backward computation.
I0802 09:37:10.786803 30954 net.cpp:172] fc6 does not need backward computation.
I0802 09:37:10.786855 30954 net.cpp:172] pool5 does not need backward computation.
I0802 09:37:10.786917 30954 net.cpp:172] relu5 does not need backward computation.
I0802 09:37:10.786973 30954 net.cpp:172] conv5 does not need backward computation.
I0802 09:37:10.787031 30954 net.cpp:172] relu4 does not need backward computation.
I0802 09:37:10.787093 30954 net.cpp:172] conv4 does not need backward computation.
I0802 09:37:10.787163 30954 net.cpp:172] relu3 does not need backward computation.
I0802 09:37:10.787225 30954 net.cpp:172] conv3 does not need backward computation.
I0802 09:37:10.787276 30954 net.cpp:172] norm2 does not need backward computation.
I0802 09:37:10.787335 30954 net.cpp:172] pool2 does not need backward computation.
I0802 09:37:10.787390 30954 net.cpp:172] relu2 does not need backward computation.
I0802 09:37:10.787459 30954 net.cpp:172] conv2 does not need backward computation.
I0802 09:37:10.787523 30954 net.cpp:172] norm1 does not need backward computation.
I0802 09:37:10.787578 30954 net.cpp:172] pool1 does not need backward computation.
I0802 09:37:10.787758 30954 net.cpp:172] relu1 does not need backward computation.
I0802 09:37:10.787837 30954 net.cpp:172] conv1 does not need backward computation.
I0802 09:37:10.787904 30954 net.cpp:208] This network produces output argmax
I0802 09:37:10.788164 30954 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 09:37:10.788362 30954 net.cpp:219] Network initialization done.
I0802 09:37:10.788419 30954 net.cpp:220] Memory required for data: 6249796
E0802 09:37:12.998558 30954 upgrade_proto.cpp:615] Attempting to upgrade input file specified using deprecated transformation parameters: ../../common/weights/imc.caffemodel
I0802 09:37:12.999415 30954 upgrade_proto.cpp:618] Successfully upgraded file specified using deprecated data transformation parameters.
E0802 09:37:12.999480 30954 upgrade_proto.cpp:620] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0802 09:37:13.411226 30954 img-client.cpp:139] Reading input/imc/dog-208-227.jpg
I0802 09:37:13.420969 30954 img-client.cpp:139] Reading input/imc/cat-281-227.jpg
I0802 09:37:13.431067 30954 img-client.cpp:139] Reading input/imc/tp-999-227.jpg
I0802 09:37:13.438762 30954 tonic.cpp:31] Reshaping input to dims: 3 3 227 227
I0802 09:37:13.448122 30954 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 09:55:19.947252 30954 img-client.cpp:222] Image: input/imc/dog-208-227.jpg class: 208
I0802 09:55:19.952392 30954 img-client.cpp:222] Image: input/imc/cat-281-227.jpg class: 281
I0802 09:55:19.953440 30954 img-client.cpp:222] Image: input/imc/tp-999-227.jpg class: 999
634.06user 4.22system 18:10.27elapsed 58%CPU (0avgtext+0avgdata 2216512maxresident)k
0inputs+8outputs (0major+217372minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-imc
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-imc
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 09:55:31.321846 19613 net.cpp:39] Initializing net from parameters: 
name: "imc"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 1000
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227
I0802 09:55:31.323729 19613 net.cpp:358] Input 0 -> data
I0802 09:55:31.324183 19613 net.cpp:67] Creating Layer conv1
I0802 09:55:31.324268 19613 net.cpp:394] conv1 <- data
I0802 09:55:31.324339 19613 net.cpp:356] conv1 -> conv1
I0802 09:55:31.324501 19613 net.cpp:96] Setting up conv1
I0802 09:55:31.325019 19613 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:55:31.325273 19613 net.cpp:67] Creating Layer relu1
I0802 09:55:31.325336 19613 net.cpp:394] relu1 <- conv1
I0802 09:55:31.325386 19613 net.cpp:345] relu1 -> conv1 (in-place)
I0802 09:55:31.325470 19613 net.cpp:96] Setting up relu1
I0802 09:55:31.325556 19613 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 09:55:31.325634 19613 net.cpp:67] Creating Layer pool1
I0802 09:55:31.325680 19613 net.cpp:394] pool1 <- conv1
I0802 09:55:31.325717 19613 net.cpp:356] pool1 -> pool1
I0802 09:55:31.325800 19613 net.cpp:96] Setting up pool1
I0802 09:55:31.325934 19613 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:55:31.326010 19613 net.cpp:67] Creating Layer norm1
I0802 09:55:31.326056 19613 net.cpp:394] norm1 <- pool1
I0802 09:55:31.326125 19613 net.cpp:356] norm1 -> norm1
I0802 09:55:31.326205 19613 net.cpp:96] Setting up norm1
I0802 09:55:31.326279 19613 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 09:55:31.326354 19613 net.cpp:67] Creating Layer conv2
I0802 09:55:31.326403 19613 net.cpp:394] conv2 <- norm1
I0802 09:55:31.326467 19613 net.cpp:356] conv2 -> conv2
I0802 09:55:31.326544 19613 net.cpp:96] Setting up conv2
I0802 09:55:31.328321 19613 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:55:31.328802 19613 net.cpp:67] Creating Layer relu2
I0802 09:55:31.328861 19613 net.cpp:394] relu2 <- conv2
I0802 09:55:31.329018 19613 net.cpp:345] relu2 -> conv2 (in-place)
I0802 09:55:31.329130 19613 net.cpp:96] Setting up relu2
I0802 09:55:31.329200 19613 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 09:55:31.329258 19613 net.cpp:67] Creating Layer pool2
I0802 09:55:31.329323 19613 net.cpp:394] pool2 <- conv2
I0802 09:55:31.329375 19613 net.cpp:356] pool2 -> pool2
I0802 09:55:31.329447 19613 net.cpp:96] Setting up pool2
I0802 09:55:31.329538 19613 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:55:31.329623 19613 net.cpp:67] Creating Layer norm2
I0802 09:55:31.329670 19613 net.cpp:394] norm2 <- pool2
I0802 09:55:31.329733 19613 net.cpp:356] norm2 -> norm2
I0802 09:55:31.329798 19613 net.cpp:96] Setting up norm2
I0802 09:55:31.329859 19613 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:55:31.329946 19613 net.cpp:67] Creating Layer conv3
I0802 09:55:31.329994 19613 net.cpp:394] conv3 <- norm2
I0802 09:55:31.330044 19613 net.cpp:356] conv3 -> conv3
I0802 09:55:31.330112 19613 net.cpp:96] Setting up conv3
I0802 09:55:31.336395 19613 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:55:31.337098 19613 net.cpp:67] Creating Layer relu3
I0802 09:55:31.337167 19613 net.cpp:394] relu3 <- conv3
I0802 09:55:31.337352 19613 net.cpp:345] relu3 -> conv3 (in-place)
I0802 09:55:31.337445 19613 net.cpp:96] Setting up relu3
I0802 09:55:31.337493 19613 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:55:31.337565 19613 net.cpp:67] Creating Layer conv4
I0802 09:55:31.337604 19613 net.cpp:394] conv4 <- conv3
I0802 09:55:31.337677 19613 net.cpp:356] conv4 -> conv4
I0802 09:55:31.337748 19613 net.cpp:96] Setting up conv4
I0802 09:55:31.342314 19613 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:55:31.342900 19613 net.cpp:67] Creating Layer relu4
I0802 09:55:31.342957 19613 net.cpp:394] relu4 <- conv4
I0802 09:55:31.343158 19613 net.cpp:345] relu4 -> conv4 (in-place)
I0802 09:55:31.343258 19613 net.cpp:96] Setting up relu4
I0802 09:55:31.343330 19613 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 09:55:31.343396 19613 net.cpp:67] Creating Layer conv5
I0802 09:55:31.343456 19613 net.cpp:394] conv5 <- conv4
I0802 09:55:31.343525 19613 net.cpp:356] conv5 -> conv5
I0802 09:55:31.343647 19613 net.cpp:96] Setting up conv5
I0802 09:55:31.345916 19613 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:55:31.346544 19613 net.cpp:67] Creating Layer relu5
I0802 09:55:31.346601 19613 net.cpp:394] relu5 <- conv5
I0802 09:55:31.346853 19613 net.cpp:345] relu5 -> conv5 (in-place)
I0802 09:55:31.346956 19613 net.cpp:96] Setting up relu5
I0802 09:55:31.347007 19613 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 09:55:31.347082 19613 net.cpp:67] Creating Layer pool5
I0802 09:55:31.347137 19613 net.cpp:394] pool5 <- conv5
I0802 09:55:31.347187 19613 net.cpp:356] pool5 -> pool5
I0802 09:55:31.347260 19613 net.cpp:96] Setting up pool5
I0802 09:55:31.347338 19613 net.cpp:103] Top shape: 1 256 6 6 (9216)
I0802 09:55:31.347398 19613 net.cpp:67] Creating Layer fc6
I0802 09:55:31.347442 19613 net.cpp:394] fc6 <- pool5
I0802 09:55:31.347519 19613 net.cpp:356] fc6 -> fc6
I0802 09:55:31.347651 19613 net.cpp:96] Setting up fc6
I0802 09:55:31.880697 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:31.881436 19613 net.cpp:67] Creating Layer relu6
I0802 09:55:31.881536 19613 net.cpp:394] relu6 <- fc6
I0802 09:55:31.881619 19613 net.cpp:345] relu6 -> fc6 (in-place)
I0802 09:55:31.881723 19613 net.cpp:96] Setting up relu6
I0802 09:55:31.881773 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:31.881850 19613 net.cpp:67] Creating Layer drop6
I0802 09:55:31.881896 19613 net.cpp:394] drop6 <- fc6
I0802 09:55:31.881963 19613 net.cpp:345] drop6 -> fc6 (in-place)
I0802 09:55:31.882087 19613 net.cpp:96] Setting up drop6
I0802 09:55:31.882233 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:31.882294 19613 net.cpp:67] Creating Layer fc7
I0802 09:55:31.882355 19613 net.cpp:394] fc7 <- fc6
I0802 09:55:31.882468 19613 net.cpp:356] fc7 -> fc7
I0802 09:55:31.882541 19613 net.cpp:96] Setting up fc7
I0802 09:55:32.090296 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:32.091012 19613 net.cpp:67] Creating Layer relu7
I0802 09:55:32.091078 19613 net.cpp:394] relu7 <- fc7
I0802 09:55:32.091189 19613 net.cpp:345] relu7 -> fc7 (in-place)
I0802 09:55:32.091284 19613 net.cpp:96] Setting up relu7
I0802 09:55:32.091341 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:32.091413 19613 net.cpp:67] Creating Layer drop7
I0802 09:55:32.091473 19613 net.cpp:394] drop7 <- fc7
I0802 09:55:32.091542 19613 net.cpp:345] drop7 -> fc7 (in-place)
I0802 09:55:32.091668 19613 net.cpp:96] Setting up drop7
I0802 09:55:32.091744 19613 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 09:55:32.091814 19613 net.cpp:67] Creating Layer fc8
I0802 09:55:32.091877 19613 net.cpp:394] fc8 <- fc7
I0802 09:55:32.091948 19613 net.cpp:356] fc8 -> fc8
I0802 09:55:32.092034 19613 net.cpp:96] Setting up fc8
I0802 09:55:32.138627 19613 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:55:32.141947 19613 net.cpp:67] Creating Layer prob
I0802 09:55:32.142393 19613 net.cpp:394] prob <- fc8
I0802 09:55:32.142485 19613 net.cpp:356] prob -> prob
I0802 09:55:32.142704 19613 net.cpp:96] Setting up prob
I0802 09:55:32.144930 19613 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 09:55:32.145385 19613 net.cpp:67] Creating Layer argmax
I0802 09:55:32.145566 19613 net.cpp:394] argmax <- prob
I0802 09:55:32.145689 19613 net.cpp:356] argmax -> argmax
I0802 09:55:32.145812 19613 net.cpp:96] Setting up argmax
I0802 09:55:32.145887 19613 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 09:55:32.145941 19613 net.cpp:172] argmax does not need backward computation.
I0802 09:55:32.146040 19613 net.cpp:172] prob does not need backward computation.
I0802 09:55:32.146100 19613 net.cpp:172] fc8 does not need backward computation.
I0802 09:55:32.146154 19613 net.cpp:172] drop7 does not need backward computation.
I0802 09:55:32.146210 19613 net.cpp:172] relu7 does not need backward computation.
I0802 09:55:32.146262 19613 net.cpp:172] fc7 does not need backward computation.
I0802 09:55:32.146324 19613 net.cpp:172] drop6 does not need backward computation.
I0802 09:55:32.146366 19613 net.cpp:172] relu6 does not need backward computation.
I0802 09:55:32.146427 19613 net.cpp:172] fc6 does not need backward computation.
I0802 09:55:32.146479 19613 net.cpp:172] pool5 does not need backward computation.
I0802 09:55:32.146533 19613 net.cpp:172] relu5 does not need backward computation.
I0802 09:55:32.146591 19613 net.cpp:172] conv5 does not need backward computation.
I0802 09:55:32.146648 19613 net.cpp:172] relu4 does not need backward computation.
I0802 09:55:32.146708 19613 net.cpp:172] conv4 does not need backward computation.
I0802 09:55:32.146766 19613 net.cpp:172] relu3 does not need backward computation.
I0802 09:55:32.146826 19613 net.cpp:172] conv3 does not need backward computation.
I0802 09:55:32.146885 19613 net.cpp:172] norm2 does not need backward computation.
I0802 09:55:32.146945 19613 net.cpp:172] pool2 does not need backward computation.
I0802 09:55:32.147002 19613 net.cpp:172] relu2 does not need backward computation.
I0802 09:55:32.147063 19613 net.cpp:172] conv2 does not need backward computation.
I0802 09:55:32.147126 19613 net.cpp:172] norm1 does not need backward computation.
I0802 09:55:32.147183 19613 net.cpp:172] pool1 does not need backward computation.
I0802 09:55:32.147258 19613 net.cpp:172] relu1 does not need backward computation.
I0802 09:55:32.147321 19613 net.cpp:172] conv1 does not need backward computation.
I0802 09:55:32.147377 19613 net.cpp:208] This network produces output argmax
I0802 09:55:32.147562 19613 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 09:55:32.147858 19613 net.cpp:219] Network initialization done.
I0802 09:55:32.147912 19613 net.cpp:220] Memory required for data: 6249796
E0802 09:55:34.397233 19613 upgrade_proto.cpp:615] Attempting to upgrade input file specified using deprecated transformation parameters: ../../common/weights/imc.caffemodel
I0802 09:55:34.402751 19613 upgrade_proto.cpp:618] Successfully upgraded file specified using deprecated data transformation parameters.
E0802 09:55:34.403043 19613 upgrade_proto.cpp:620] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0802 09:55:34.814977 19613 img-client.cpp:139] Reading input/imc/dog-208-227.jpg
I0802 09:55:34.820173 19613 img-client.cpp:139] Reading input/imc/cat-281-227.jpg
I0802 09:55:34.824448 19613 img-client.cpp:139] Reading input/imc/tp-999-227.jpg
I0802 09:55:34.835536 19613 tonic.cpp:31] Reshaping input to dims: 3 3 227 227
I0802 09:55:34.838585 19613 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 10:13:49.307133 19613 img-client.cpp:222] Image: input/imc/dog-208-227.jpg class: 208
I0802 10:13:49.314478 19613 img-client.cpp:222] Image: input/imc/cat-281-227.jpg class: 281
I0802 10:13:49.314627 19613 img-client.cpp:222] Image: input/imc/tp-999-227.jpg class: 999
637.85user 4.43system 18:18.36elapsed 58%CPU (0avgtext+0avgdata 2216464maxresident)k
0inputs+8outputs (0major+217372minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-imc
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-imc
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 10:14:00.805909  8541 net.cpp:39] Initializing net from parameters: 
name: "imc"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 1000
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227
I0802 10:14:00.807747  8541 net.cpp:358] Input 0 -> data
I0802 10:14:00.808225  8541 net.cpp:67] Creating Layer conv1
I0802 10:14:00.808305  8541 net.cpp:394] conv1 <- data
I0802 10:14:00.808377  8541 net.cpp:356] conv1 -> conv1
I0802 10:14:00.808562  8541 net.cpp:96] Setting up conv1
I0802 10:14:00.809074  8541 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 10:14:00.809320  8541 net.cpp:67] Creating Layer relu1
I0802 10:14:00.809376  8541 net.cpp:394] relu1 <- conv1
I0802 10:14:00.809463  8541 net.cpp:345] relu1 -> conv1 (in-place)
I0802 10:14:00.809516  8541 net.cpp:96] Setting up relu1
I0802 10:14:00.809597  8541 net.cpp:103] Top shape: 1 96 55 55 (290400)
I0802 10:14:00.809672  8541 net.cpp:67] Creating Layer pool1
I0802 10:14:00.809720  8541 net.cpp:394] pool1 <- conv1
I0802 10:14:00.809772  8541 net.cpp:356] pool1 -> pool1
I0802 10:14:00.809857  8541 net.cpp:96] Setting up pool1
I0802 10:14:00.810005  8541 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 10:14:00.810091  8541 net.cpp:67] Creating Layer norm1
I0802 10:14:00.810138  8541 net.cpp:394] norm1 <- pool1
I0802 10:14:00.810195  8541 net.cpp:356] norm1 -> norm1
I0802 10:14:00.810291  8541 net.cpp:96] Setting up norm1
I0802 10:14:00.810382  8541 net.cpp:103] Top shape: 1 96 27 27 (69984)
I0802 10:14:00.810452  8541 net.cpp:67] Creating Layer conv2
I0802 10:14:00.810513  8541 net.cpp:394] conv2 <- norm1
I0802 10:14:00.810570  8541 net.cpp:356] conv2 -> conv2
I0802 10:14:00.810647  8541 net.cpp:96] Setting up conv2
I0802 10:14:00.812491  8541 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 10:14:00.813060  8541 net.cpp:67] Creating Layer relu2
I0802 10:14:00.813125  8541 net.cpp:394] relu2 <- conv2
I0802 10:14:00.813307  8541 net.cpp:345] relu2 -> conv2 (in-place)
I0802 10:14:00.813412  8541 net.cpp:96] Setting up relu2
I0802 10:14:00.813498  8541 net.cpp:103] Top shape: 1 256 27 27 (186624)
I0802 10:14:00.813555  8541 net.cpp:67] Creating Layer pool2
I0802 10:14:00.813607  8541 net.cpp:394] pool2 <- conv2
I0802 10:14:00.813657  8541 net.cpp:356] pool2 -> pool2
I0802 10:14:00.813731  8541 net.cpp:96] Setting up pool2
I0802 10:14:00.813789  8541 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 10:14:00.813886  8541 net.cpp:67] Creating Layer norm2
I0802 10:14:00.813933  8541 net.cpp:394] norm2 <- pool2
I0802 10:14:00.813987  8541 net.cpp:356] norm2 -> norm2
I0802 10:14:00.814081  8541 net.cpp:96] Setting up norm2
I0802 10:14:00.814155  8541 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 10:14:00.814234  8541 net.cpp:67] Creating Layer conv3
I0802 10:14:00.814295  8541 net.cpp:394] conv3 <- norm2
I0802 10:14:00.814352  8541 net.cpp:356] conv3 -> conv3
I0802 10:14:00.814488  8541 net.cpp:96] Setting up conv3
I0802 10:14:00.820731  8541 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 10:14:00.821480  8541 net.cpp:67] Creating Layer relu3
I0802 10:14:00.821534  8541 net.cpp:394] relu3 <- conv3
I0802 10:14:00.821734  8541 net.cpp:345] relu3 -> conv3 (in-place)
I0802 10:14:00.821830  8541 net.cpp:96] Setting up relu3
I0802 10:14:00.821882  8541 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 10:14:00.821940  8541 net.cpp:67] Creating Layer conv4
I0802 10:14:00.821995  8541 net.cpp:394] conv4 <- conv3
I0802 10:14:00.822055  8541 net.cpp:356] conv4 -> conv4
I0802 10:14:00.822145  8541 net.cpp:96] Setting up conv4
I0802 10:14:00.827219  8541 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 10:14:00.827986  8541 net.cpp:67] Creating Layer relu4
I0802 10:14:00.828047  8541 net.cpp:394] relu4 <- conv4
I0802 10:14:00.828244  8541 net.cpp:345] relu4 -> conv4 (in-place)
I0802 10:14:00.828341  8541 net.cpp:96] Setting up relu4
I0802 10:14:00.828393  8541 net.cpp:103] Top shape: 1 384 13 13 (64896)
I0802 10:14:00.828479  8541 net.cpp:67] Creating Layer conv5
I0802 10:14:00.828521  8541 net.cpp:394] conv5 <- conv4
I0802 10:14:00.828589  8541 net.cpp:356] conv5 -> conv5
I0802 10:14:00.828649  8541 net.cpp:96] Setting up conv5
I0802 10:14:00.831418  8541 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 10:14:00.832293  8541 net.cpp:67] Creating Layer relu5
I0802 10:14:00.832375  8541 net.cpp:394] relu5 <- conv5
I0802 10:14:00.832502  8541 net.cpp:345] relu5 -> conv5 (in-place)
I0802 10:14:00.832623  8541 net.cpp:96] Setting up relu5
I0802 10:14:00.832675  8541 net.cpp:103] Top shape: 1 256 13 13 (43264)
I0802 10:14:00.832725  8541 net.cpp:67] Creating Layer pool5
I0802 10:14:00.832800  8541 net.cpp:394] pool5 <- conv5
I0802 10:14:00.832869  8541 net.cpp:356] pool5 -> pool5
I0802 10:14:00.832937  8541 net.cpp:96] Setting up pool5
I0802 10:14:00.833020  8541 net.cpp:103] Top shape: 1 256 6 6 (9216)
I0802 10:14:00.833091  8541 net.cpp:67] Creating Layer fc6
I0802 10:14:00.833149  8541 net.cpp:394] fc6 <- pool5
I0802 10:14:00.833209  8541 net.cpp:356] fc6 -> fc6
I0802 10:14:00.833343  8541 net.cpp:96] Setting up fc6
I0802 10:14:01.360414  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.361120  8541 net.cpp:67] Creating Layer relu6
I0802 10:14:01.361177  8541 net.cpp:394] relu6 <- fc6
I0802 10:14:01.361351  8541 net.cpp:345] relu6 -> fc6 (in-place)
I0802 10:14:01.361452  8541 net.cpp:96] Setting up relu6
I0802 10:14:01.361511  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.361588  8541 net.cpp:67] Creating Layer drop6
I0802 10:14:01.361634  8541 net.cpp:394] drop6 <- fc6
I0802 10:14:01.361680  8541 net.cpp:345] drop6 -> fc6 (in-place)
I0802 10:14:01.361774  8541 net.cpp:96] Setting up drop6
I0802 10:14:01.361889  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.361950  8541 net.cpp:67] Creating Layer fc7
I0802 10:14:01.361991  8541 net.cpp:394] fc7 <- fc6
I0802 10:14:01.362077  8541 net.cpp:356] fc7 -> fc7
I0802 10:14:01.362138  8541 net.cpp:96] Setting up fc7
I0802 10:14:01.569713  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.570441  8541 net.cpp:67] Creating Layer relu7
I0802 10:14:01.570529  8541 net.cpp:394] relu7 <- fc7
I0802 10:14:01.570663  8541 net.cpp:345] relu7 -> fc7 (in-place)
I0802 10:14:01.570793  8541 net.cpp:96] Setting up relu7
I0802 10:14:01.570870  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.570943  8541 net.cpp:67] Creating Layer drop7
I0802 10:14:01.570999  8541 net.cpp:394] drop7 <- fc7
I0802 10:14:01.571081  8541 net.cpp:345] drop7 -> fc7 (in-place)
I0802 10:14:01.571180  8541 net.cpp:96] Setting up drop7
I0802 10:14:01.571229  8541 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 10:14:01.571291  8541 net.cpp:67] Creating Layer fc8
I0802 10:14:01.571344  8541 net.cpp:394] fc8 <- fc7
I0802 10:14:01.571411  8541 net.cpp:356] fc8 -> fc8
I0802 10:14:01.571504  8541 net.cpp:96] Setting up fc8
I0802 10:14:01.630981  8541 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 10:14:01.634552  8541 net.cpp:67] Creating Layer prob
I0802 10:14:01.634999  8541 net.cpp:394] prob <- fc8
I0802 10:14:01.635298  8541 net.cpp:356] prob -> prob
I0802 10:14:01.635407  8541 net.cpp:96] Setting up prob
I0802 10:14:01.635563  8541 net.cpp:103] Top shape: 1 1000 1 1 (1000)
I0802 10:14:01.635795  8541 net.cpp:67] Creating Layer argmax
I0802 10:14:01.635846  8541 net.cpp:394] argmax <- prob
I0802 10:14:01.635934  8541 net.cpp:356] argmax -> argmax
I0802 10:14:01.635993  8541 net.cpp:96] Setting up argmax
I0802 10:14:01.636061  8541 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 10:14:01.636118  8541 net.cpp:172] argmax does not need backward computation.
I0802 10:14:01.636210  8541 net.cpp:172] prob does not need backward computation.
I0802 10:14:01.636270  8541 net.cpp:172] fc8 does not need backward computation.
I0802 10:14:01.636327  8541 net.cpp:172] drop7 does not need backward computation.
I0802 10:14:01.636382  8541 net.cpp:172] relu7 does not need backward computation.
I0802 10:14:01.636436  8541 net.cpp:172] fc7 does not need backward computation.
I0802 10:14:01.636492  8541 net.cpp:172] drop6 does not need backward computation.
I0802 10:14:01.636549  8541 net.cpp:172] relu6 does not need backward computation.
I0802 10:14:01.636602  8541 net.cpp:172] fc6 does not need backward computation.
I0802 10:14:01.636659  8541 net.cpp:172] pool5 does not need backward computation.
I0802 10:14:01.636714  8541 net.cpp:172] relu5 does not need backward computation.
I0802 10:14:01.636768  8541 net.cpp:172] conv5 does not need backward computation.
I0802 10:14:01.636824  8541 net.cpp:172] relu4 does not need backward computation.
I0802 10:14:01.636875  8541 net.cpp:172] conv4 does not need backward computation.
I0802 10:14:01.636929  8541 net.cpp:172] relu3 does not need backward computation.
I0802 10:14:01.636986  8541 net.cpp:172] conv3 does not need backward computation.
I0802 10:14:01.637039  8541 net.cpp:172] norm2 does not need backward computation.
I0802 10:14:01.637094  8541 net.cpp:172] pool2 does not need backward computation.
I0802 10:14:01.637147  8541 net.cpp:172] relu2 does not need backward computation.
I0802 10:14:01.637925  8541 net.cpp:172] conv2 does not need backward computation.
I0802 10:14:01.638020  8541 net.cpp:172] norm1 does not need backward computation.
I0802 10:14:01.638094  8541 net.cpp:172] pool1 does not need backward computation.
I0802 10:14:01.638164  8541 net.cpp:172] relu1 does not need backward computation.
I0802 10:14:01.638217  8541 net.cpp:172] conv1 does not need backward computation.
I0802 10:14:01.638269  8541 net.cpp:208] This network produces output argmax
I0802 10:14:01.638453  8541 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 10:14:01.638592  8541 net.cpp:219] Network initialization done.
I0802 10:14:01.638641  8541 net.cpp:220] Memory required for data: 6249796
E0802 10:14:03.914960  8541 upgrade_proto.cpp:615] Attempting to upgrade input file specified using deprecated transformation parameters: ../../common/weights/imc.caffemodel
I0802 10:14:03.921540  8541 upgrade_proto.cpp:618] Successfully upgraded file specified using deprecated data transformation parameters.
E0802 10:14:03.921974  8541 upgrade_proto.cpp:620] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0802 10:14:04.371450  8541 img-client.cpp:139] Reading input/imc/dog-208-227.jpg
I0802 10:14:04.382796  8541 img-client.cpp:139] Reading input/imc/cat-281-227.jpg
I0802 10:14:04.387027  8541 img-client.cpp:139] Reading input/imc/tp-999-227.jpg
I0802 10:14:04.398331  8541 tonic.cpp:31] Reshaping input to dims: 3 3 227 227
I0802 10:14:04.398828  8541 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 10:32:27.792493  8541 img-client.cpp:222] Image: input/imc/dog-208-227.jpg class: 208
I0802 10:32:27.799250  8541 img-client.cpp:222] Image: input/imc/cat-281-227.jpg class: 281
I0802 10:32:27.800159  8541 img-client.cpp:222] Image: input/imc/tp-999-227.jpg class: 999
643.34user 4.18system 18:27.34elapsed 58%CPU (0avgtext+0avgdata 2216528maxresident)k
0inputs+8outputs (0major+217372minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-dig
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-dig
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 10:32:39.227408 30684 net.cpp:39] Initializing net from parameters: 
name: "dig"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 28
input_dim: 28
I0802 10:32:39.229074 30684 net.cpp:358] Input 0 -> data
I0802 10:32:39.229480 30684 net.cpp:67] Creating Layer conv1
I0802 10:32:39.229555 30684 net.cpp:394] conv1 <- data
I0802 10:32:39.229634 30684 net.cpp:356] conv1 -> conv1
I0802 10:32:39.229883 30684 net.cpp:96] Setting up conv1
I0802 10:32:39.230857 30684 net.cpp:103] Top shape: 1 20 24 24 (11520)
I0802 10:32:39.231173 30684 net.cpp:67] Creating Layer pool1
I0802 10:32:39.231235 30684 net.cpp:394] pool1 <- conv1
I0802 10:32:39.231335 30684 net.cpp:356] pool1 -> pool1
I0802 10:32:39.231432 30684 net.cpp:96] Setting up pool1
I0802 10:32:39.231539 30684 net.cpp:103] Top shape: 1 20 12 12 (2880)
I0802 10:32:39.231681 30684 net.cpp:67] Creating Layer conv2
I0802 10:32:39.231739 30684 net.cpp:394] conv2 <- pool1
I0802 10:32:39.231791 30684 net.cpp:356] conv2 -> conv2
I0802 10:32:39.231859 30684 net.cpp:96] Setting up conv2
I0802 10:32:39.232295 30684 net.cpp:103] Top shape: 1 50 8 8 (3200)
I0802 10:32:39.232417 30684 net.cpp:67] Creating Layer pool2
I0802 10:32:39.232465 30684 net.cpp:394] pool2 <- conv2
I0802 10:32:39.232532 30684 net.cpp:356] pool2 -> pool2
I0802 10:32:39.232581 30684 net.cpp:96] Setting up pool2
I0802 10:32:39.232648 30684 net.cpp:103] Top shape: 1 50 4 4 (800)
I0802 10:32:39.232722 30684 net.cpp:67] Creating Layer ip1
I0802 10:32:39.232777 30684 net.cpp:394] ip1 <- pool2
I0802 10:32:39.232834 30684 net.cpp:356] ip1 -> ip1
I0802 10:32:39.232935 30684 net.cpp:96] Setting up ip1
I0802 10:32:39.238935 30684 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 10:32:39.239747 30684 net.cpp:67] Creating Layer relu1
I0802 10:32:39.239816 30684 net.cpp:394] relu1 <- ip1
I0802 10:32:39.239997 30684 net.cpp:345] relu1 -> ip1 (in-place)
I0802 10:32:39.240102 30684 net.cpp:96] Setting up relu1
I0802 10:32:39.240223 30684 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 10:32:39.240304 30684 net.cpp:67] Creating Layer ip2
I0802 10:32:39.240350 30684 net.cpp:394] ip2 <- ip1
I0802 10:32:39.240418 30684 net.cpp:356] ip2 -> ip2
I0802 10:32:39.240489 30684 net.cpp:96] Setting up ip2
I0802 10:32:39.240721 30684 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 10:32:39.240816 30684 net.cpp:67] Creating Layer prob
I0802 10:32:39.240893 30684 net.cpp:394] prob <- ip2
I0802 10:32:39.240952 30684 net.cpp:356] prob -> prob
I0802 10:32:39.241029 30684 net.cpp:96] Setting up prob
I0802 10:32:39.241096 30684 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 10:32:39.241179 30684 net.cpp:67] Creating Layer argmax
I0802 10:32:39.241228 30684 net.cpp:394] argmax <- prob
I0802 10:32:39.241305 30684 net.cpp:356] argmax -> argmax
I0802 10:32:39.241390 30684 net.cpp:96] Setting up argmax
I0802 10:32:39.241498 30684 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 10:32:39.241550 30684 net.cpp:172] argmax does not need backward computation.
I0802 10:32:39.241641 30684 net.cpp:172] prob does not need backward computation.
I0802 10:32:39.241684 30684 net.cpp:172] ip2 does not need backward computation.
I0802 10:32:39.241742 30684 net.cpp:172] relu1 does not need backward computation.
I0802 10:32:39.241789 30684 net.cpp:172] ip1 does not need backward computation.
I0802 10:32:39.241847 30684 net.cpp:172] pool2 does not need backward computation.
I0802 10:32:39.241899 30684 net.cpp:172] conv2 does not need backward computation.
I0802 10:32:39.241943 30684 net.cpp:172] pool1 does not need backward computation.
I0802 10:32:39.241997 30684 net.cpp:172] conv1 does not need backward computation.
I0802 10:32:39.242069 30684 net.cpp:208] This network produces output argmax
I0802 10:32:39.242208 30684 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 10:32:39.242336 30684 net.cpp:219] Network initialization done.
I0802 10:32:39.242388 30684 net.cpp:220] Memory required for data: 77684
I0802 10:32:39.270710 30684 img-client.cpp:139] Reading input/dig/0.png
I0802 10:32:39.278132 30684 img-client.cpp:139] Reading input/dig/1.png
I0802 10:32:39.282491 30684 img-client.cpp:139] Reading input/dig/2.png
I0802 10:32:39.287139 30684 img-client.cpp:139] Reading input/dig/3.png
I0802 10:32:39.288950 30684 img-client.cpp:139] Reading input/dig/4.png
I0802 10:32:39.292564 30684 img-client.cpp:139] Reading input/dig/5.png
I0802 10:32:39.294206 30684 img-client.cpp:139] Reading input/dig/6.png
I0802 10:32:39.294960 30684 img-client.cpp:139] Reading input/dig/7.png
I0802 10:32:39.295500 30684 img-client.cpp:139] Reading input/dig/8.png
I0802 10:32:39.296202 30684 img-client.cpp:139] Reading input/dig/9.png
I0802 10:32:39.298705 30684 tonic.cpp:31] Reshaping input to dims: 10 1 28 28
I0802 10:32:39.299141 30684 tonic.cpp:37] Reshaping output to dims: 10 1 1 1
I0802 10:52:02.789690 30684 img-client.cpp:222] Image: input/dig/0.png class: 0
I0802 10:52:02.792371 30684 img-client.cpp:222] Image: input/dig/1.png class: 1
I0802 10:52:02.792516 30684 img-client.cpp:222] Image: input/dig/2.png class: 2
I0802 10:52:02.792553 30684 img-client.cpp:222] Image: input/dig/3.png class: 3
I0802 10:52:02.792598 30684 img-client.cpp:222] Image: input/dig/4.png class: 4
I0802 10:52:02.792691 30684 img-client.cpp:222] Image: input/dig/5.png class: 5
I0802 10:52:02.792779 30684 img-client.cpp:222] Image: input/dig/6.png class: 6
I0802 10:52:02.792839 30684 img-client.cpp:222] Image: input/dig/7.png class: 7
I0802 10:52:02.792891 30684 img-client.cpp:222] Image: input/dig/8.png class: 8
I0802 10:52:02.792981 30684 img-client.cpp:222] Image: input/dig/9.png class: 9
674.23user 4.24system 19:23.80elapsed 58%CPU (0avgtext+0avgdata 177728maxresident)k
4064inputs+8outputs (2major+11939minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-dig
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-dig
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 10:52:14.079542 23838 net.cpp:39] Initializing net from parameters: 
name: "dig"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 28
input_dim: 28
I0802 10:52:14.086046 23838 net.cpp:358] Input 0 -> data
I0802 10:52:14.086431 23838 net.cpp:67] Creating Layer conv1
I0802 10:52:14.086515 23838 net.cpp:394] conv1 <- data
I0802 10:52:14.086783 23838 net.cpp:356] conv1 -> conv1
I0802 10:52:14.086976 23838 net.cpp:96] Setting up conv1
I0802 10:52:14.088045 23838 net.cpp:103] Top shape: 1 20 24 24 (11520)
I0802 10:52:14.088366 23838 net.cpp:67] Creating Layer pool1
I0802 10:52:14.088428 23838 net.cpp:394] pool1 <- conv1
I0802 10:52:14.088488 23838 net.cpp:356] pool1 -> pool1
I0802 10:52:14.088639 23838 net.cpp:96] Setting up pool1
I0802 10:52:14.088768 23838 net.cpp:103] Top shape: 1 20 12 12 (2880)
I0802 10:52:14.088853 23838 net.cpp:67] Creating Layer conv2
I0802 10:52:14.088909 23838 net.cpp:394] conv2 <- pool1
I0802 10:52:14.088979 23838 net.cpp:356] conv2 -> conv2
I0802 10:52:14.089045 23838 net.cpp:96] Setting up conv2
I0802 10:52:14.089584 23838 net.cpp:103] Top shape: 1 50 8 8 (3200)
I0802 10:52:14.089716 23838 net.cpp:67] Creating Layer pool2
I0802 10:52:14.089786 23838 net.cpp:394] pool2 <- conv2
I0802 10:52:14.089846 23838 net.cpp:356] pool2 -> pool2
I0802 10:52:14.089929 23838 net.cpp:96] Setting up pool2
I0802 10:52:14.089984 23838 net.cpp:103] Top shape: 1 50 4 4 (800)
I0802 10:52:14.090154 23838 net.cpp:67] Creating Layer ip1
I0802 10:52:14.090201 23838 net.cpp:394] ip1 <- pool2
I0802 10:52:14.090282 23838 net.cpp:356] ip1 -> ip1
I0802 10:52:14.090373 23838 net.cpp:96] Setting up ip1
I0802 10:52:14.096753 23838 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 10:52:14.097539 23838 net.cpp:67] Creating Layer relu1
I0802 10:52:14.097630 23838 net.cpp:394] relu1 <- ip1
I0802 10:52:14.097702 23838 net.cpp:345] relu1 -> ip1 (in-place)
I0802 10:52:14.097805 23838 net.cpp:96] Setting up relu1
I0802 10:52:14.097921 23838 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 10:52:14.097986 23838 net.cpp:67] Creating Layer ip2
I0802 10:52:14.098032 23838 net.cpp:394] ip2 <- ip1
I0802 10:52:14.098103 23838 net.cpp:356] ip2 -> ip2
I0802 10:52:14.098183 23838 net.cpp:96] Setting up ip2
I0802 10:52:14.098464 23838 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 10:52:14.098569 23838 net.cpp:67] Creating Layer prob
I0802 10:52:14.098633 23838 net.cpp:394] prob <- ip2
I0802 10:52:14.098697 23838 net.cpp:356] prob -> prob
I0802 10:52:14.098770 23838 net.cpp:96] Setting up prob
I0802 10:52:14.098836 23838 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 10:52:14.098913 23838 net.cpp:67] Creating Layer argmax
I0802 10:52:14.098963 23838 net.cpp:394] argmax <- prob
I0802 10:52:14.099046 23838 net.cpp:356] argmax -> argmax
I0802 10:52:14.099105 23838 net.cpp:96] Setting up argmax
I0802 10:52:14.099215 23838 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 10:52:14.099266 23838 net.cpp:172] argmax does not need backward computation.
I0802 10:52:14.099366 23838 net.cpp:172] prob does not need backward computation.
I0802 10:52:14.099423 23838 net.cpp:172] ip2 does not need backward computation.
I0802 10:52:14.099494 23838 net.cpp:172] relu1 does not need backward computation.
I0802 10:52:14.099551 23838 net.cpp:172] ip1 does not need backward computation.
I0802 10:52:14.099699 23838 net.cpp:172] pool2 does not need backward computation.
I0802 10:52:14.099764 23838 net.cpp:172] conv2 does not need backward computation.
I0802 10:52:14.099817 23838 net.cpp:172] pool1 does not need backward computation.
I0802 10:52:14.099874 23838 net.cpp:172] conv1 does not need backward computation.
I0802 10:52:14.099941 23838 net.cpp:208] This network produces output argmax
I0802 10:52:14.100080 23838 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 10:52:14.100213 23838 net.cpp:219] Network initialization done.
I0802 10:52:14.100267 23838 net.cpp:220] Memory required for data: 77684
I0802 10:52:14.115847 23838 img-client.cpp:139] Reading input/dig/0.png
I0802 10:52:14.117770 23838 img-client.cpp:139] Reading input/dig/1.png
I0802 10:52:14.118113 23838 img-client.cpp:139] Reading input/dig/2.png
I0802 10:52:14.118319 23838 img-client.cpp:139] Reading input/dig/3.png
I0802 10:52:14.118486 23838 img-client.cpp:139] Reading input/dig/4.png
I0802 10:52:14.118659 23838 img-client.cpp:139] Reading input/dig/5.png
I0802 10:52:14.118815 23838 img-client.cpp:139] Reading input/dig/6.png
I0802 10:52:14.118983 23838 img-client.cpp:139] Reading input/dig/7.png
I0802 10:52:14.119133 23838 img-client.cpp:139] Reading input/dig/8.png
I0802 10:52:14.119303 23838 img-client.cpp:139] Reading input/dig/9.png
I0802 10:52:14.119680 23838 tonic.cpp:31] Reshaping input to dims: 10 1 28 28
I0802 10:52:14.119810 23838 tonic.cpp:37] Reshaping output to dims: 10 1 1 1
I0802 11:11:28.527709 23838 img-client.cpp:222] Image: input/dig/0.png class: 0
I0802 11:11:28.529688 23838 img-client.cpp:222] Image: input/dig/1.png class: 1
I0802 11:11:28.529793 23838 img-client.cpp:222] Image: input/dig/2.png class: 2
I0802 11:11:28.529831 23838 img-client.cpp:222] Image: input/dig/3.png class: 3
I0802 11:11:28.529881 23838 img-client.cpp:222] Image: input/dig/4.png class: 4
I0802 11:11:28.530045 23838 img-client.cpp:222] Image: input/dig/5.png class: 5
I0802 11:11:28.530128 23838 img-client.cpp:222] Image: input/dig/6.png class: 6
I0802 11:11:28.530182 23838 img-client.cpp:222] Image: input/dig/7.png class: 7
I0802 11:11:28.530254 23838 img-client.cpp:222] Image: input/dig/8.png class: 8
I0802 11:11:28.530349 23838 img-client.cpp:222] Image: input/dig/9.png class: 9
669.08user 4.43system 19:14.68elapsed 58%CPU (0avgtext+0avgdata 177712maxresident)k
0inputs+8outputs (0major+11941minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-dig
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-dig
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 11:11:39.830078 16627 net.cpp:39] Initializing net from parameters: 
name: "dig"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 28
input_dim: 28
I0802 11:11:39.831791 16627 net.cpp:358] Input 0 -> data
I0802 11:11:39.832252 16627 net.cpp:67] Creating Layer conv1
I0802 11:11:39.832331 16627 net.cpp:394] conv1 <- data
I0802 11:11:39.832417 16627 net.cpp:356] conv1 -> conv1
I0802 11:11:39.832593 16627 net.cpp:96] Setting up conv1
I0802 11:11:39.833708 16627 net.cpp:103] Top shape: 1 20 24 24 (11520)
I0802 11:11:39.834033 16627 net.cpp:67] Creating Layer pool1
I0802 11:11:39.834086 16627 net.cpp:394] pool1 <- conv1
I0802 11:11:39.834182 16627 net.cpp:356] pool1 -> pool1
I0802 11:11:39.834283 16627 net.cpp:96] Setting up pool1
I0802 11:11:39.834394 16627 net.cpp:103] Top shape: 1 20 12 12 (2880)
I0802 11:11:39.834460 16627 net.cpp:67] Creating Layer conv2
I0802 11:11:39.834522 16627 net.cpp:394] conv2 <- pool1
I0802 11:11:39.834570 16627 net.cpp:356] conv2 -> conv2
I0802 11:11:39.834642 16627 net.cpp:96] Setting up conv2
I0802 11:11:39.835192 16627 net.cpp:103] Top shape: 1 50 8 8 (3200)
I0802 11:11:39.835326 16627 net.cpp:67] Creating Layer pool2
I0802 11:11:39.835378 16627 net.cpp:394] pool2 <- conv2
I0802 11:11:39.835453 16627 net.cpp:356] pool2 -> pool2
I0802 11:11:39.835508 16627 net.cpp:96] Setting up pool2
I0802 11:11:39.835574 16627 net.cpp:103] Top shape: 1 50 4 4 (800)
I0802 11:11:39.835741 16627 net.cpp:67] Creating Layer ip1
I0802 11:11:39.835803 16627 net.cpp:394] ip1 <- pool2
I0802 11:11:39.835861 16627 net.cpp:356] ip1 -> ip1
I0802 11:11:39.835963 16627 net.cpp:96] Setting up ip1
I0802 11:11:39.841889 16627 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:11:39.842641 16627 net.cpp:67] Creating Layer relu1
I0802 11:11:39.842702 16627 net.cpp:394] relu1 <- ip1
I0802 11:11:39.842775 16627 net.cpp:345] relu1 -> ip1 (in-place)
I0802 11:11:39.842968 16627 net.cpp:96] Setting up relu1
I0802 11:11:39.843092 16627 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:11:39.843169 16627 net.cpp:67] Creating Layer ip2
I0802 11:11:39.843214 16627 net.cpp:394] ip2 <- ip1
I0802 11:11:39.843277 16627 net.cpp:356] ip2 -> ip2
I0802 11:11:39.843350 16627 net.cpp:96] Setting up ip2
I0802 11:11:39.843571 16627 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:11:39.843741 16627 net.cpp:67] Creating Layer prob
I0802 11:11:39.843803 16627 net.cpp:394] prob <- ip2
I0802 11:11:39.843852 16627 net.cpp:356] prob -> prob
I0802 11:11:39.843909 16627 net.cpp:96] Setting up prob
I0802 11:11:39.843971 16627 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:11:39.844053 16627 net.cpp:67] Creating Layer argmax
I0802 11:11:39.844104 16627 net.cpp:394] argmax <- prob
I0802 11:11:39.844172 16627 net.cpp:356] argmax -> argmax
I0802 11:11:39.844235 16627 net.cpp:96] Setting up argmax
I0802 11:11:39.844368 16627 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 11:11:39.844416 16627 net.cpp:172] argmax does not need backward computation.
I0802 11:11:39.844490 16627 net.cpp:172] prob does not need backward computation.
I0802 11:11:39.844540 16627 net.cpp:172] ip2 does not need backward computation.
I0802 11:11:39.844583 16627 net.cpp:172] relu1 does not need backward computation.
I0802 11:11:39.844645 16627 net.cpp:172] ip1 does not need backward computation.
I0802 11:11:39.844698 16627 net.cpp:172] pool2 does not need backward computation.
I0802 11:11:39.844743 16627 net.cpp:172] conv2 does not need backward computation.
I0802 11:11:39.844808 16627 net.cpp:172] pool1 does not need backward computation.
I0802 11:11:39.844862 16627 net.cpp:172] conv1 does not need backward computation.
I0802 11:11:39.844931 16627 net.cpp:208] This network produces output argmax
I0802 11:11:39.845063 16627 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 11:11:39.845273 16627 net.cpp:219] Network initialization done.
I0802 11:11:39.845329 16627 net.cpp:220] Memory required for data: 77684
I0802 11:11:39.857897 16627 img-client.cpp:139] Reading input/dig/0.png
I0802 11:11:39.859865 16627 img-client.cpp:139] Reading input/dig/1.png
I0802 11:11:39.860306 16627 img-client.cpp:139] Reading input/dig/2.png
I0802 11:11:39.860548 16627 img-client.cpp:139] Reading input/dig/3.png
I0802 11:11:39.860707 16627 img-client.cpp:139] Reading input/dig/4.png
I0802 11:11:39.860867 16627 img-client.cpp:139] Reading input/dig/5.png
I0802 11:11:39.861017 16627 img-client.cpp:139] Reading input/dig/6.png
I0802 11:11:39.861207 16627 img-client.cpp:139] Reading input/dig/7.png
I0802 11:11:39.861371 16627 img-client.cpp:139] Reading input/dig/8.png
I0802 11:11:39.861572 16627 img-client.cpp:139] Reading input/dig/9.png
I0802 11:11:39.861910 16627 tonic.cpp:31] Reshaping input to dims: 10 1 28 28
I0802 11:11:39.862035 16627 tonic.cpp:37] Reshaping output to dims: 10 1 1 1
I0802 11:30:57.898867 16627 img-client.cpp:222] Image: input/dig/0.png class: 0
I0802 11:30:57.907341 16627 img-client.cpp:222] Image: input/dig/1.png class: 1
I0802 11:30:57.907881 16627 img-client.cpp:222] Image: input/dig/2.png class: 2
I0802 11:30:57.907922 16627 img-client.cpp:222] Image: input/dig/3.png class: 3
I0802 11:30:57.907969 16627 img-client.cpp:222] Image: input/dig/4.png class: 4
I0802 11:30:57.908103 16627 img-client.cpp:222] Image: input/dig/5.png class: 5
I0802 11:30:57.908161 16627 img-client.cpp:222] Image: input/dig/6.png class: 6
I0802 11:30:57.908231 16627 img-client.cpp:222] Image: input/dig/7.png class: 7
I0802 11:30:57.908288 16627 img-client.cpp:222] Image: input/dig/8.png class: 8
I0802 11:30:57.908377 16627 img-client.cpp:222] Image: input/dig/9.png class: 9
671.41user 4.37system 19:18.30elapsed 58%CPU (0avgtext+0avgdata 177712maxresident)k
0inputs+8outputs (0major+11940minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-dig
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-dig
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 11:31:09.242636  9852 net.cpp:39] Initializing net from parameters: 
name: "dig"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 28
input_dim: 28
I0802 11:31:09.244189  9852 net.cpp:358] Input 0 -> data
I0802 11:31:09.244560  9852 net.cpp:67] Creating Layer conv1
I0802 11:31:09.244632  9852 net.cpp:394] conv1 <- data
I0802 11:31:09.244714  9852 net.cpp:356] conv1 -> conv1
I0802 11:31:09.244856  9852 net.cpp:96] Setting up conv1
I0802 11:31:09.245848  9852 net.cpp:103] Top shape: 1 20 24 24 (11520)
I0802 11:31:09.246176  9852 net.cpp:67] Creating Layer pool1
I0802 11:31:09.246243  9852 net.cpp:394] pool1 <- conv1
I0802 11:31:09.246287  9852 net.cpp:356] pool1 -> pool1
I0802 11:31:09.246366  9852 net.cpp:96] Setting up pool1
I0802 11:31:09.246533  9852 net.cpp:103] Top shape: 1 20 12 12 (2880)
I0802 11:31:09.246601  9852 net.cpp:67] Creating Layer conv2
I0802 11:31:09.246645  9852 net.cpp:394] conv2 <- pool1
I0802 11:31:09.246696  9852 net.cpp:356] conv2 -> conv2
I0802 11:31:09.246770  9852 net.cpp:96] Setting up conv2
I0802 11:31:09.247269  9852 net.cpp:103] Top shape: 1 50 8 8 (3200)
I0802 11:31:09.247392  9852 net.cpp:67] Creating Layer pool2
I0802 11:31:09.247462  9852 net.cpp:394] pool2 <- conv2
I0802 11:31:09.247514  9852 net.cpp:356] pool2 -> pool2
I0802 11:31:09.247575  9852 net.cpp:96] Setting up pool2
I0802 11:31:09.247725  9852 net.cpp:103] Top shape: 1 50 4 4 (800)
I0802 11:31:09.247808  9852 net.cpp:67] Creating Layer ip1
I0802 11:31:09.247864  9852 net.cpp:394] ip1 <- pool2
I0802 11:31:09.247941  9852 net.cpp:356] ip1 -> ip1
I0802 11:31:09.248018  9852 net.cpp:96] Setting up ip1
I0802 11:31:09.253937  9852 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:31:09.254694  9852 net.cpp:67] Creating Layer relu1
I0802 11:31:09.254763  9852 net.cpp:394] relu1 <- ip1
I0802 11:31:09.254828  9852 net.cpp:345] relu1 -> ip1 (in-place)
I0802 11:31:09.254935  9852 net.cpp:96] Setting up relu1
I0802 11:31:09.255070  9852 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:31:09.255142  9852 net.cpp:67] Creating Layer ip2
I0802 11:31:09.255190  9852 net.cpp:394] ip2 <- ip1
I0802 11:31:09.255257  9852 net.cpp:356] ip2 -> ip2
I0802 11:31:09.255332  9852 net.cpp:96] Setting up ip2
I0802 11:31:09.255555  9852 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:31:09.255719  9852 net.cpp:67] Creating Layer prob
I0802 11:31:09.255785  9852 net.cpp:394] prob <- ip2
I0802 11:31:09.255846  9852 net.cpp:356] prob -> prob
I0802 11:31:09.255914  9852 net.cpp:96] Setting up prob
I0802 11:31:09.255982  9852 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:31:09.256065  9852 net.cpp:67] Creating Layer argmax
I0802 11:31:09.256110  9852 net.cpp:394] argmax <- prob
I0802 11:31:09.256189  9852 net.cpp:356] argmax -> argmax
I0802 11:31:09.256240  9852 net.cpp:96] Setting up argmax
I0802 11:31:09.256384  9852 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 11:31:09.256433  9852 net.cpp:172] argmax does not need backward computation.
I0802 11:31:09.256515  9852 net.cpp:172] prob does not need backward computation.
I0802 11:31:09.256561  9852 net.cpp:172] ip2 does not need backward computation.
I0802 11:31:09.256621  9852 net.cpp:172] relu1 does not need backward computation.
I0802 11:31:09.256666  9852 net.cpp:172] ip1 does not need backward computation.
I0802 11:31:09.256726  9852 net.cpp:172] pool2 does not need backward computation.
I0802 11:31:09.256768  9852 net.cpp:172] conv2 does not need backward computation.
I0802 11:31:09.256826  9852 net.cpp:172] pool1 does not need backward computation.
I0802 11:31:09.256873  9852 net.cpp:172] conv1 does not need backward computation.
I0802 11:31:09.256928  9852 net.cpp:208] This network produces output argmax
I0802 11:31:09.257056  9852 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 11:31:09.257185  9852 net.cpp:219] Network initialization done.
I0802 11:31:09.257233  9852 net.cpp:220] Memory required for data: 77684
I0802 11:31:09.270673  9852 img-client.cpp:139] Reading input/dig/0.png
I0802 11:31:09.272783  9852 img-client.cpp:139] Reading input/dig/1.png
I0802 11:31:09.273298  9852 img-client.cpp:139] Reading input/dig/2.png
I0802 11:31:09.273480  9852 img-client.cpp:139] Reading input/dig/3.png
I0802 11:31:09.273736  9852 img-client.cpp:139] Reading input/dig/4.png
I0802 11:31:09.273915  9852 img-client.cpp:139] Reading input/dig/5.png
I0802 11:31:09.274062  9852 img-client.cpp:139] Reading input/dig/6.png
I0802 11:31:09.274226  9852 img-client.cpp:139] Reading input/dig/7.png
I0802 11:31:09.274374  9852 img-client.cpp:139] Reading input/dig/8.png
I0802 11:31:09.274538  9852 img-client.cpp:139] Reading input/dig/9.png
I0802 11:31:09.274893  9852 tonic.cpp:31] Reshaping input to dims: 10 1 28 28
I0802 11:31:09.275022  9852 tonic.cpp:37] Reshaping output to dims: 10 1 1 1
I0802 11:50:23.027956  9852 img-client.cpp:222] Image: input/dig/0.png class: 0
I0802 11:50:23.037703  9852 img-client.cpp:222] Image: input/dig/1.png class: 1
I0802 11:50:23.038250  9852 img-client.cpp:222] Image: input/dig/2.png class: 2
I0802 11:50:23.038301  9852 img-client.cpp:222] Image: input/dig/3.png class: 3
I0802 11:50:23.038352  9852 img-client.cpp:222] Image: input/dig/4.png class: 4
I0802 11:50:23.038533  9852 img-client.cpp:222] Image: input/dig/5.png class: 5
I0802 11:50:23.038596  9852 img-client.cpp:222] Image: input/dig/6.png class: 6
I0802 11:50:23.038640  9852 img-client.cpp:222] Image: input/dig/7.png class: 7
I0802 11:50:23.038702  9852 img-client.cpp:222] Image: input/dig/8.png class: 8
I0802 11:50:23.038789  9852 img-client.cpp:222] Image: input/dig/9.png class: 9
668.74user 4.05system 19:14.08elapsed 58%CPU (0avgtext+0avgdata 177664maxresident)k
0inputs+8outputs (0major+11940minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-dig
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-dig
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 11:50:34.321105  2391 net.cpp:39] Initializing net from parameters: 
name: "dig"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 28
input_dim: 28
I0802 11:50:34.324066  2391 net.cpp:358] Input 0 -> data
I0802 11:50:34.324584  2391 net.cpp:67] Creating Layer conv1
I0802 11:50:34.324673  2391 net.cpp:394] conv1 <- data
I0802 11:50:34.324782  2391 net.cpp:356] conv1 -> conv1
I0802 11:50:34.324956  2391 net.cpp:96] Setting up conv1
I0802 11:50:34.326236  2391 net.cpp:103] Top shape: 1 20 24 24 (11520)
I0802 11:50:34.326572  2391 net.cpp:67] Creating Layer pool1
I0802 11:50:34.326635  2391 net.cpp:394] pool1 <- conv1
I0802 11:50:34.326690  2391 net.cpp:356] pool1 -> pool1
I0802 11:50:34.326824  2391 net.cpp:96] Setting up pool1
I0802 11:50:34.326961  2391 net.cpp:103] Top shape: 1 20 12 12 (2880)
I0802 11:50:34.327044  2391 net.cpp:67] Creating Layer conv2
I0802 11:50:34.327095  2391 net.cpp:394] conv2 <- pool1
I0802 11:50:34.327146  2391 net.cpp:356] conv2 -> conv2
I0802 11:50:34.327214  2391 net.cpp:96] Setting up conv2
I0802 11:50:34.327808  2391 net.cpp:103] Top shape: 1 50 8 8 (3200)
I0802 11:50:34.327954  2391 net.cpp:67] Creating Layer pool2
I0802 11:50:34.328002  2391 net.cpp:394] pool2 <- conv2
I0802 11:50:34.328075  2391 net.cpp:356] pool2 -> pool2
I0802 11:50:34.328130  2391 net.cpp:96] Setting up pool2
I0802 11:50:34.328193  2391 net.cpp:103] Top shape: 1 50 4 4 (800)
I0802 11:50:34.328260  2391 net.cpp:67] Creating Layer ip1
I0802 11:50:34.328323  2391 net.cpp:394] ip1 <- pool2
I0802 11:50:34.328388  2391 net.cpp:356] ip1 -> ip1
I0802 11:50:34.328500  2391 net.cpp:96] Setting up ip1
I0802 11:50:34.334885  2391 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:50:34.335512  2391 net.cpp:67] Creating Layer relu1
I0802 11:50:34.335569  2391 net.cpp:394] relu1 <- ip1
I0802 11:50:34.335958  2391 net.cpp:345] relu1 -> ip1 (in-place)
I0802 11:50:34.336067  2391 net.cpp:96] Setting up relu1
I0802 11:50:34.336191  2391 net.cpp:103] Top shape: 1 500 1 1 (500)
I0802 11:50:34.336257  2391 net.cpp:67] Creating Layer ip2
I0802 11:50:34.336302  2391 net.cpp:394] ip2 <- ip1
I0802 11:50:34.336371  2391 net.cpp:356] ip2 -> ip2
I0802 11:50:34.336457  2391 net.cpp:96] Setting up ip2
I0802 11:50:34.336748  2391 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:50:34.336863  2391 net.cpp:67] Creating Layer prob
I0802 11:50:34.336912  2391 net.cpp:394] prob <- ip2
I0802 11:50:34.336984  2391 net.cpp:356] prob -> prob
I0802 11:50:34.337054  2391 net.cpp:96] Setting up prob
I0802 11:50:34.337133  2391 net.cpp:103] Top shape: 1 10 1 1 (10)
I0802 11:50:34.337229  2391 net.cpp:67] Creating Layer argmax
I0802 11:50:34.337283  2391 net.cpp:394] argmax <- prob
I0802 11:50:34.337347  2391 net.cpp:356] argmax -> argmax
I0802 11:50:34.337415  2391 net.cpp:96] Setting up argmax
I0802 11:50:34.337504  2391 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 11:50:34.337553  2391 net.cpp:172] argmax does not need backward computation.
I0802 11:50:34.337627  2391 net.cpp:172] prob does not need backward computation.
I0802 11:50:34.337667  2391 net.cpp:172] ip2 does not need backward computation.
I0802 11:50:34.337730  2391 net.cpp:172] relu1 does not need backward computation.
I0802 11:50:34.337793  2391 net.cpp:172] ip1 does not need backward computation.
I0802 11:50:34.337839  2391 net.cpp:172] pool2 does not need backward computation.
I0802 11:50:34.337894  2391 net.cpp:172] conv2 does not need backward computation.
I0802 11:50:34.337936  2391 net.cpp:172] pool1 does not need backward computation.
I0802 11:50:34.337990  2391 net.cpp:172] conv1 does not need backward computation.
I0802 11:50:34.338032  2391 net.cpp:208] This network produces output argmax
I0802 11:50:34.338176  2391 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 11:50:34.338304  2391 net.cpp:219] Network initialization done.
I0802 11:50:34.338348  2391 net.cpp:220] Memory required for data: 77684
I0802 11:50:34.351055  2391 img-client.cpp:139] Reading input/dig/0.png
I0802 11:50:34.352939  2391 img-client.cpp:139] Reading input/dig/1.png
I0802 11:50:34.353406  2391 img-client.cpp:139] Reading input/dig/2.png
I0802 11:50:34.353571  2391 img-client.cpp:139] Reading input/dig/3.png
I0802 11:50:34.353735  2391 img-client.cpp:139] Reading input/dig/4.png
I0802 11:50:34.353893  2391 img-client.cpp:139] Reading input/dig/5.png
I0802 11:50:34.354048  2391 img-client.cpp:139] Reading input/dig/6.png
I0802 11:50:34.354189  2391 img-client.cpp:139] Reading input/dig/7.png
I0802 11:50:34.354357  2391 img-client.cpp:139] Reading input/dig/8.png
I0802 11:50:34.354519  2391 img-client.cpp:139] Reading input/dig/9.png
I0802 11:50:34.354835  2391 tonic.cpp:31] Reshaping input to dims: 10 1 28 28
I0802 11:50:34.354953  2391 tonic.cpp:37] Reshaping output to dims: 10 1 1 1
I0802 12:09:47.085053  2391 img-client.cpp:222] Image: input/dig/0.png class: 0
I0802 12:09:47.087385  2391 img-client.cpp:222] Image: input/dig/1.png class: 1
I0802 12:09:47.087494  2391 img-client.cpp:222] Image: input/dig/2.png class: 2
I0802 12:09:47.087533  2391 img-client.cpp:222] Image: input/dig/3.png class: 3
I0802 12:09:47.087579  2391 img-client.cpp:222] Image: input/dig/4.png class: 4
I0802 12:09:47.087946  2391 img-client.cpp:222] Image: input/dig/5.png class: 5
I0802 12:09:47.088027  2391 img-client.cpp:222] Image: input/dig/6.png class: 6
I0802 12:09:47.088095  2391 img-client.cpp:222] Image: input/dig/7.png class: 7
I0802 12:09:47.088141  2391 img-client.cpp:222] Image: input/dig/8.png class: 8
I0802 12:09:47.088235  2391 img-client.cpp:222] Image: input/dig/9.png class: 9
668.22user 4.00system 19:13.02elapsed 58%CPU (0avgtext+0avgdata 177712maxresident)k
0inputs+8outputs (0major+11940minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-face
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-face
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 12:09:58.434960 26995 net.cpp:39] Initializing net from parameters: 
name: "face"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 1
  }
}
layers {
  bottom: "conv1"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "conv3"
  top: "local4"
  name: "local4"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "local4"
  top: "local5"
  name: "local5"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 7
    stride: 2
  }
}
layers {
  bottom: "local5"
  top: "local6"
  name: "local6"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 5
    stride: 1
  }
}
layers {
  bottom: "local6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 83
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 152
input_dim: 152
I0802 12:09:58.436775 26995 net.cpp:358] Input 0 -> data
I0802 12:09:58.437221 26995 net.cpp:67] Creating Layer conv1
I0802 12:09:58.437311 26995 net.cpp:394] conv1 <- data
I0802 12:09:58.437391 26995 net.cpp:356] conv1 -> conv1
I0802 12:09:58.437543 26995 net.cpp:96] Setting up conv1
I0802 12:09:58.438268 26995 net.cpp:103] Top shape: 1 32 142 142 (645248)
I0802 12:09:58.438515 26995 net.cpp:67] Creating Layer pool2
I0802 12:09:58.438565 26995 net.cpp:394] pool2 <- conv1
I0802 12:09:58.438634 26995 net.cpp:356] pool2 -> pool2
I0802 12:09:58.438714 26995 net.cpp:96] Setting up pool2
I0802 12:09:58.438848 26995 net.cpp:103] Top shape: 1 32 71 71 (161312)
I0802 12:09:58.438911 26995 net.cpp:67] Creating Layer conv3
I0802 12:09:58.438977 26995 net.cpp:394] conv3 <- pool2
I0802 12:09:58.439030 26995 net.cpp:356] conv3 -> conv3
I0802 12:09:58.439102 26995 net.cpp:96] Setting up conv3
I0802 12:09:58.439657 26995 net.cpp:103] Top shape: 1 16 63 63 (63504)
I0802 12:09:58.439777 26995 net.cpp:67] Creating Layer local4
I0802 12:09:58.439839 26995 net.cpp:394] local4 <- conv3
I0802 12:09:58.439930 26995 net.cpp:356] local4 -> local4
I0802 12:09:58.440006 26995 net.cpp:96] Setting up local4
I0802 12:10:00.190579 26995 net.cpp:103] Top shape: 1 16 55 55 (48400)
I0802 12:10:00.191277 26995 net.cpp:67] Creating Layer local5
I0802 12:10:00.191347 26995 net.cpp:394] local5 <- local4
I0802 12:10:00.191442 26995 net.cpp:356] local5 -> local5
I0802 12:10:00.191542 26995 net.cpp:96] Setting up local5
I0802 12:10:00.379338 26995 net.cpp:103] Top shape: 1 16 25 25 (10000)
I0802 12:10:00.380206 26995 net.cpp:67] Creating Layer local6
I0802 12:10:00.380281 26995 net.cpp:394] local6 <- local5
I0802 12:10:00.380389 26995 net.cpp:356] local6 -> local6
I0802 12:10:00.380508 26995 net.cpp:96] Setting up local6
I0802 12:10:00.418390 26995 net.cpp:103] Top shape: 1 16 21 21 (7056)
I0802 12:10:00.419049 26995 net.cpp:67] Creating Layer fc7
I0802 12:10:00.419136 26995 net.cpp:394] fc7 <- local6
I0802 12:10:00.419255 26995 net.cpp:356] fc7 -> fc7
I0802 12:10:00.419415 26995 net.cpp:96] Setting up fc7
I0802 12:10:01.193517 26995 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 12:10:01.197190 26995 net.cpp:67] Creating Layer fc8
I0802 12:10:01.197679 26995 net.cpp:394] fc8 <- fc7
I0802 12:10:01.197883 26995 net.cpp:356] fc8 -> fc8
I0802 12:10:01.198051 26995 net.cpp:96] Setting up fc8
I0802 12:10:01.211554 26995 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:10:01.215013 26995 net.cpp:67] Creating Layer prob
I0802 12:10:01.215478 26995 net.cpp:394] prob <- fc8
I0802 12:10:01.215781 26995 net.cpp:356] prob -> prob
I0802 12:10:01.215911 26995 net.cpp:96] Setting up prob
I0802 12:10:01.216032 26995 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:10:01.216101 26995 net.cpp:67] Creating Layer argmax
I0802 12:10:01.216142 26995 net.cpp:394] argmax <- prob
I0802 12:10:01.216184 26995 net.cpp:356] argmax -> argmax
I0802 12:10:01.216254 26995 net.cpp:96] Setting up argmax
I0802 12:10:01.216327 26995 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 12:10:01.216382 26995 net.cpp:172] argmax does not need backward computation.
I0802 12:10:01.216482 26995 net.cpp:172] prob does not need backward computation.
I0802 12:10:01.216541 26995 net.cpp:172] fc8 does not need backward computation.
I0802 12:10:01.216598 26995 net.cpp:172] fc7 does not need backward computation.
I0802 12:10:01.216653 26995 net.cpp:172] local6 does not need backward computation.
I0802 12:10:01.216713 26995 net.cpp:172] local5 does not need backward computation.
I0802 12:10:01.216756 26995 net.cpp:172] local4 does not need backward computation.
I0802 12:10:01.216820 26995 net.cpp:172] conv3 does not need backward computation.
I0802 12:10:01.216879 26995 net.cpp:172] pool2 does not need backward computation.
I0802 12:10:01.216967 26995 net.cpp:172] conv1 does not need backward computation.
I0802 12:10:01.217012 26995 net.cpp:208] This network produces output argmax
I0802 12:10:01.217183 26995 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 12:10:01.217341 26995 net.cpp:219] Network initialization done.
I0802 12:10:01.217396 26995 net.cpp:220] Memory required for data: 3759132
I0802 12:10:08.638535 26995 img-client.cpp:139] Reading input/face/cameron-diaz-11-152.jpg
I0802 12:10:08.641785 26995 img-client.cpp:139] Reading input/face/angelina-jolie-03-152.jpg
I0802 12:10:08.644387 26995 img-client.cpp:139] Reading input/face/daniel-craig-20-152.jpg
I0802 12:10:08.647110 26995 img-client.cpp:161] Aligning: input/face/cameron-diaz-11-152.jpg
I0802 12:10:08.842842 26995 img-client.cpp:161] Aligning: input/face/angelina-jolie-03-152.jpg
I0802 12:10:08.989675 26995 img-client.cpp:161] Aligning: input/face/daniel-craig-20-152.jpg
I0802 12:10:09.106899 26995 tonic.cpp:31] Reshaping input to dims: 3 3 152 152
I0802 12:10:09.107861 26995 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 12:31:17.924549 26995 img-client.cpp:222] Image: input/face/cameron-diaz-11-152.jpg class: 11
I0802 12:31:17.926412 26995 img-client.cpp:222] Image: input/face/angelina-jolie-03-152.jpg class: 0
I0802 12:31:17.926504 26995 img-client.cpp:222] Image: input/face/daniel-craig-20-152.jpg class: 0
752.19user 7.39system 21:19.91elapsed 59%CPU (0avgtext+0avgdata 3593424maxresident)k
818040inputs+8outputs (17major+342244minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-face
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-face
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 12:31:29.406546  3351 net.cpp:39] Initializing net from parameters: 
name: "face"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 1
  }
}
layers {
  bottom: "conv1"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "conv3"
  top: "local4"
  name: "local4"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "local4"
  top: "local5"
  name: "local5"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 7
    stride: 2
  }
}
layers {
  bottom: "local5"
  top: "local6"
  name: "local6"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 5
    stride: 1
  }
}
layers {
  bottom: "local6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 83
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 152
input_dim: 152
I0802 12:31:29.408320  3351 net.cpp:358] Input 0 -> data
I0802 12:31:29.408740  3351 net.cpp:67] Creating Layer conv1
I0802 12:31:29.408818  3351 net.cpp:394] conv1 <- data
I0802 12:31:29.408913  3351 net.cpp:356] conv1 -> conv1
I0802 12:31:29.409065  3351 net.cpp:96] Setting up conv1
I0802 12:31:29.409668  3351 net.cpp:103] Top shape: 1 32 142 142 (645248)
I0802 12:31:29.409909  3351 net.cpp:67] Creating Layer pool2
I0802 12:31:29.409963  3351 net.cpp:394] pool2 <- conv1
I0802 12:31:29.410043  3351 net.cpp:356] pool2 -> pool2
I0802 12:31:29.410126  3351 net.cpp:96] Setting up pool2
I0802 12:31:29.410272  3351 net.cpp:103] Top shape: 1 32 71 71 (161312)
I0802 12:31:29.410336  3351 net.cpp:67] Creating Layer conv3
I0802 12:31:29.410395  3351 net.cpp:394] conv3 <- pool2
I0802 12:31:29.410442  3351 net.cpp:356] conv3 -> conv3
I0802 12:31:29.410512  3351 net.cpp:96] Setting up conv3
I0802 12:31:29.410961  3351 net.cpp:103] Top shape: 1 16 63 63 (63504)
I0802 12:31:29.411056  3351 net.cpp:67] Creating Layer local4
I0802 12:31:29.411115  3351 net.cpp:394] local4 <- conv3
I0802 12:31:29.411195  3351 net.cpp:356] local4 -> local4
I0802 12:31:29.411280  3351 net.cpp:96] Setting up local4
I0802 12:31:30.241364  3351 net.cpp:103] Top shape: 1 16 55 55 (48400)
I0802 12:31:30.248090  3351 net.cpp:67] Creating Layer local5
I0802 12:31:30.248519  3351 net.cpp:394] local5 <- local4
I0802 12:31:30.248692  3351 net.cpp:356] local5 -> local5
I0802 12:31:30.248795  3351 net.cpp:96] Setting up local5
I0802 12:31:30.332221  3351 net.cpp:103] Top shape: 1 16 25 25 (10000)
I0802 12:31:30.333077  3351 net.cpp:67] Creating Layer local6
I0802 12:31:30.333154  3351 net.cpp:394] local6 <- local5
I0802 12:31:30.333259  3351 net.cpp:356] local6 -> local6
I0802 12:31:30.333382  3351 net.cpp:96] Setting up local6
I0802 12:31:30.368968  3351 net.cpp:103] Top shape: 1 16 21 21 (7056)
I0802 12:31:30.369722  3351 net.cpp:67] Creating Layer fc7
I0802 12:31:30.369792  3351 net.cpp:394] fc7 <- local6
I0802 12:31:30.369894  3351 net.cpp:356] fc7 -> fc7
I0802 12:31:30.370043  3351 net.cpp:96] Setting up fc7
I0802 12:31:30.699576  3351 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 12:31:30.702905  3351 net.cpp:67] Creating Layer fc8
I0802 12:31:30.702994  3351 net.cpp:394] fc8 <- fc7
I0802 12:31:30.703104  3351 net.cpp:356] fc8 -> fc8
I0802 12:31:30.703236  3351 net.cpp:96] Setting up fc8
I0802 12:31:30.704843  3351 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:31:30.709380  3351 net.cpp:67] Creating Layer prob
I0802 12:31:30.709789  3351 net.cpp:394] prob <- fc8
I0802 12:31:30.709969  3351 net.cpp:356] prob -> prob
I0802 12:31:30.711084  3351 net.cpp:96] Setting up prob
I0802 12:31:30.711278  3351 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:31:30.711385  3351 net.cpp:67] Creating Layer argmax
I0802 12:31:30.711436  3351 net.cpp:394] argmax <- prob
I0802 12:31:30.711516  3351 net.cpp:356] argmax -> argmax
I0802 12:31:30.711571  3351 net.cpp:96] Setting up argmax
I0802 12:31:30.711768  3351 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 12:31:30.711822  3351 net.cpp:172] argmax does not need backward computation.
I0802 12:31:30.712481  3351 net.cpp:172] prob does not need backward computation.
I0802 12:31:30.712545  3351 net.cpp:172] fc8 does not need backward computation.
I0802 12:31:30.712625  3351 net.cpp:172] fc7 does not need backward computation.
I0802 12:31:30.712672  3351 net.cpp:172] local6 does not need backward computation.
I0802 12:31:30.712731  3351 net.cpp:172] local5 does not need backward computation.
I0802 12:31:30.712787  3351 net.cpp:172] local4 does not need backward computation.
I0802 12:31:30.712872  3351 net.cpp:172] conv3 does not need backward computation.
I0802 12:31:30.712936  3351 net.cpp:172] pool2 does not need backward computation.
I0802 12:31:30.712998  3351 net.cpp:172] conv1 does not need backward computation.
I0802 12:31:30.713057  3351 net.cpp:208] This network produces output argmax
I0802 12:31:30.713255  3351 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 12:31:30.713397  3351 net.cpp:219] Network initialization done.
I0802 12:31:30.714794  3351 net.cpp:220] Memory required for data: 3759132
I0802 12:31:34.975075  3351 img-client.cpp:139] Reading input/face/cameron-diaz-11-152.jpg
I0802 12:31:34.977325  3351 img-client.cpp:139] Reading input/face/angelina-jolie-03-152.jpg
I0802 12:31:34.978742  3351 img-client.cpp:139] Reading input/face/daniel-craig-20-152.jpg
I0802 12:31:34.979969  3351 img-client.cpp:161] Aligning: input/face/cameron-diaz-11-152.jpg
I0802 12:31:35.140588  3351 img-client.cpp:161] Aligning: input/face/angelina-jolie-03-152.jpg
I0802 12:31:35.289258  3351 img-client.cpp:161] Aligning: input/face/daniel-craig-20-152.jpg
I0802 12:31:35.435885  3351 tonic.cpp:31] Reshaping input to dims: 3 3 152 152
I0802 12:31:35.436600  3351 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 12:51:59.946118  3351 img-client.cpp:222] Image: input/face/cameron-diaz-11-152.jpg class: 11
I0802 12:51:59.948494  3351 img-client.cpp:222] Image: input/face/angelina-jolie-03-152.jpg class: 0
I0802 12:51:59.948612  3351 img-client.cpp:222] Image: input/face/daniel-craig-20-152.jpg class: 0
727.02user 4.91system 20:30.97elapsed 59%CPU (0avgtext+0avgdata 3593504maxresident)k
0inputs+8outputs (0major+342261minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-face
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-face
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 12:52:11.445946  9180 net.cpp:39] Initializing net from parameters: 
name: "face"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 1
  }
}
layers {
  bottom: "conv1"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "conv3"
  top: "local4"
  name: "local4"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "local4"
  top: "local5"
  name: "local5"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 7
    stride: 2
  }
}
layers {
  bottom: "local5"
  top: "local6"
  name: "local6"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 5
    stride: 1
  }
}
layers {
  bottom: "local6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 83
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 152
input_dim: 152
I0802 12:52:11.447319  9180 net.cpp:358] Input 0 -> data
I0802 12:52:11.447710  9180 net.cpp:67] Creating Layer conv1
I0802 12:52:11.447787  9180 net.cpp:394] conv1 <- data
I0802 12:52:11.447859  9180 net.cpp:356] conv1 -> conv1
I0802 12:52:11.447993  9180 net.cpp:96] Setting up conv1
I0802 12:52:11.448635  9180 net.cpp:103] Top shape: 1 32 142 142 (645248)
I0802 12:52:11.448833  9180 net.cpp:67] Creating Layer pool2
I0802 12:52:11.448882  9180 net.cpp:394] pool2 <- conv1
I0802 12:52:11.448922  9180 net.cpp:356] pool2 -> pool2
I0802 12:52:11.448994  9180 net.cpp:96] Setting up pool2
I0802 12:52:11.449111  9180 net.cpp:103] Top shape: 1 32 71 71 (161312)
I0802 12:52:11.449173  9180 net.cpp:67] Creating Layer conv3
I0802 12:52:11.449204  9180 net.cpp:394] conv3 <- pool2
I0802 12:52:11.449245  9180 net.cpp:356] conv3 -> conv3
I0802 12:52:11.449303  9180 net.cpp:96] Setting up conv3
I0802 12:52:11.449630  9180 net.cpp:103] Top shape: 1 16 63 63 (63504)
I0802 12:52:11.449702  9180 net.cpp:67] Creating Layer local4
I0802 12:52:11.449748  9180 net.cpp:394] local4 <- conv3
I0802 12:52:11.449812  9180 net.cpp:356] local4 -> local4
I0802 12:52:11.449883  9180 net.cpp:96] Setting up local4
I0802 12:52:12.243130  9180 net.cpp:103] Top shape: 1 16 55 55 (48400)
I0802 12:52:12.252182  9180 net.cpp:67] Creating Layer local5
I0802 12:52:12.252650  9180 net.cpp:394] local5 <- local4
I0802 12:52:12.252833  9180 net.cpp:356] local5 -> local5
I0802 12:52:12.252960  9180 net.cpp:96] Setting up local5
I0802 12:52:12.358013  9180 net.cpp:103] Top shape: 1 16 25 25 (10000)
I0802 12:52:12.358754  9180 net.cpp:67] Creating Layer local6
I0802 12:52:12.358820  9180 net.cpp:394] local6 <- local5
I0802 12:52:12.358906  9180 net.cpp:356] local6 -> local6
I0802 12:52:12.359020  9180 net.cpp:96] Setting up local6
I0802 12:52:12.378367  9180 net.cpp:103] Top shape: 1 16 21 21 (7056)
I0802 12:52:12.379216  9180 net.cpp:67] Creating Layer fc7
I0802 12:52:12.379281  9180 net.cpp:394] fc7 <- local6
I0802 12:52:12.379374  9180 net.cpp:356] fc7 -> fc7
I0802 12:52:12.379523  9180 net.cpp:96] Setting up fc7
I0802 12:52:12.722848  9180 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 12:52:12.723543  9180 net.cpp:67] Creating Layer fc8
I0802 12:52:12.723675  9180 net.cpp:394] fc8 <- fc7
I0802 12:52:12.723784  9180 net.cpp:356] fc8 -> fc8
I0802 12:52:12.723922  9180 net.cpp:96] Setting up fc8
I0802 12:52:12.725579  9180 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:52:12.726315  9180 net.cpp:67] Creating Layer prob
I0802 12:52:12.726388  9180 net.cpp:394] prob <- fc8
I0802 12:52:12.726454  9180 net.cpp:356] prob -> prob
I0802 12:52:12.726593  9180 net.cpp:96] Setting up prob
I0802 12:52:12.726716  9180 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 12:52:12.726776  9180 net.cpp:67] Creating Layer argmax
I0802 12:52:12.726835  9180 net.cpp:394] argmax <- prob
I0802 12:52:12.726912  9180 net.cpp:356] argmax -> argmax
I0802 12:52:12.726969  9180 net.cpp:96] Setting up argmax
I0802 12:52:12.727038  9180 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 12:52:12.727100  9180 net.cpp:172] argmax does not need backward computation.
I0802 12:52:12.727201  9180 net.cpp:172] prob does not need backward computation.
I0802 12:52:12.727277  9180 net.cpp:172] fc8 does not need backward computation.
I0802 12:52:12.727335  9180 net.cpp:172] fc7 does not need backward computation.
I0802 12:52:12.727399  9180 net.cpp:172] local6 does not need backward computation.
I0802 12:52:12.727458  9180 net.cpp:172] local5 does not need backward computation.
I0802 12:52:12.727516  9180 net.cpp:172] local4 does not need backward computation.
I0802 12:52:12.727571  9180 net.cpp:172] conv3 does not need backward computation.
I0802 12:52:12.727696  9180 net.cpp:172] pool2 does not need backward computation.
I0802 12:52:12.727761  9180 net.cpp:172] conv1 does not need backward computation.
I0802 12:52:12.727823  9180 net.cpp:208] This network produces output argmax
I0802 12:52:12.727973  9180 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 12:52:12.728103  9180 net.cpp:219] Network initialization done.
I0802 12:52:12.728152  9180 net.cpp:220] Memory required for data: 3759132
I0802 12:52:16.540388  9180 img-client.cpp:139] Reading input/face/cameron-diaz-11-152.jpg
I0802 12:52:16.545096  9180 img-client.cpp:139] Reading input/face/angelina-jolie-03-152.jpg
I0802 12:52:16.546636  9180 img-client.cpp:139] Reading input/face/daniel-craig-20-152.jpg
I0802 12:52:16.547911  9180 img-client.cpp:161] Aligning: input/face/cameron-diaz-11-152.jpg
I0802 12:52:16.711688  9180 img-client.cpp:161] Aligning: input/face/angelina-jolie-03-152.jpg
I0802 12:52:16.861248  9180 img-client.cpp:161] Aligning: input/face/daniel-craig-20-152.jpg
I0802 12:52:16.978793  9180 tonic.cpp:31] Reshaping input to dims: 3 3 152 152
I0802 12:52:16.982909  9180 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 13:11:29.223748  9180 img-client.cpp:222] Image: input/face/cameron-diaz-11-152.jpg class: 11
I0802 13:11:29.229867  9180 img-client.cpp:222] Image: input/face/angelina-jolie-03-152.jpg class: 0
I0802 13:11:29.229924  9180 img-client.cpp:222] Image: input/face/daniel-craig-20-152.jpg class: 0
682.07user 4.54system 19:18.03elapsed 59%CPU (0avgtext+0avgdata 3593488maxresident)k
0inputs+8outputs (0major+342261minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-face
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-face
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 13:11:40.519119  7612 net.cpp:39] Initializing net from parameters: 
name: "face"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 1
  }
}
layers {
  bottom: "conv1"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "conv3"
  top: "local4"
  name: "local4"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "local4"
  top: "local5"
  name: "local5"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 7
    stride: 2
  }
}
layers {
  bottom: "local5"
  top: "local6"
  name: "local6"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 5
    stride: 1
  }
}
layers {
  bottom: "local6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 83
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 152
input_dim: 152
I0802 13:11:40.520941  7612 net.cpp:358] Input 0 -> data
I0802 13:11:40.521522  7612 net.cpp:67] Creating Layer conv1
I0802 13:11:40.521634  7612 net.cpp:394] conv1 <- data
I0802 13:11:40.521724  7612 net.cpp:356] conv1 -> conv1
I0802 13:11:40.521921  7612 net.cpp:96] Setting up conv1
I0802 13:11:40.522544  7612 net.cpp:103] Top shape: 1 32 142 142 (645248)
I0802 13:11:40.522835  7612 net.cpp:67] Creating Layer pool2
I0802 13:11:40.522897  7612 net.cpp:394] pool2 <- conv1
I0802 13:11:40.523005  7612 net.cpp:356] pool2 -> pool2
I0802 13:11:40.523110  7612 net.cpp:96] Setting up pool2
I0802 13:11:40.523239  7612 net.cpp:103] Top shape: 1 32 71 71 (161312)
I0802 13:11:40.523321  7612 net.cpp:67] Creating Layer conv3
I0802 13:11:40.523373  7612 net.cpp:394] conv3 <- pool2
I0802 13:11:40.523425  7612 net.cpp:356] conv3 -> conv3
I0802 13:11:40.523490  7612 net.cpp:96] Setting up conv3
I0802 13:11:40.523910  7612 net.cpp:103] Top shape: 1 16 63 63 (63504)
I0802 13:11:40.524039  7612 net.cpp:67] Creating Layer local4
I0802 13:11:40.524102  7612 net.cpp:394] local4 <- conv3
I0802 13:11:40.524180  7612 net.cpp:356] local4 -> local4
I0802 13:11:40.524248  7612 net.cpp:96] Setting up local4
I0802 13:11:41.295047  7612 net.cpp:103] Top shape: 1 16 55 55 (48400)
I0802 13:11:41.298060  7612 net.cpp:67] Creating Layer local5
I0802 13:11:41.298316  7612 net.cpp:394] local5 <- local4
I0802 13:11:41.298475  7612 net.cpp:356] local5 -> local5
I0802 13:11:41.298594  7612 net.cpp:96] Setting up local5
I0802 13:11:41.411928  7612 net.cpp:103] Top shape: 1 16 25 25 (10000)
I0802 13:11:41.412536  7612 net.cpp:67] Creating Layer local6
I0802 13:11:41.412607  7612 net.cpp:394] local6 <- local5
I0802 13:11:41.412717  7612 net.cpp:356] local6 -> local6
I0802 13:11:41.412825  7612 net.cpp:96] Setting up local6
I0802 13:11:41.438096  7612 net.cpp:103] Top shape: 1 16 21 21 (7056)
I0802 13:11:41.438733  7612 net.cpp:67] Creating Layer fc7
I0802 13:11:41.438792  7612 net.cpp:394] fc7 <- local6
I0802 13:11:41.438870  7612 net.cpp:356] fc7 -> fc7
I0802 13:11:41.439005  7612 net.cpp:96] Setting up fc7
I0802 13:11:41.766736  7612 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 13:11:41.772020  7612 net.cpp:67] Creating Layer fc8
I0802 13:11:41.772414  7612 net.cpp:394] fc8 <- fc7
I0802 13:11:41.772586  7612 net.cpp:356] fc8 -> fc8
I0802 13:11:41.772770  7612 net.cpp:96] Setting up fc8
I0802 13:11:41.774233  7612 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 13:11:41.774724  7612 net.cpp:67] Creating Layer prob
I0802 13:11:41.774785  7612 net.cpp:394] prob <- fc8
I0802 13:11:41.774855  7612 net.cpp:356] prob -> prob
I0802 13:11:41.774981  7612 net.cpp:96] Setting up prob
I0802 13:11:41.775099  7612 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 13:11:41.775177  7612 net.cpp:67] Creating Layer argmax
I0802 13:11:41.775233  7612 net.cpp:394] argmax <- prob
I0802 13:11:41.775301  7612 net.cpp:356] argmax -> argmax
I0802 13:11:41.775362  7612 net.cpp:96] Setting up argmax
I0802 13:11:41.775437  7612 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 13:11:41.775498  7612 net.cpp:172] argmax does not need backward computation.
I0802 13:11:41.775732  7612 net.cpp:172] prob does not need backward computation.
I0802 13:11:41.775791  7612 net.cpp:172] fc8 does not need backward computation.
I0802 13:11:41.775851  7612 net.cpp:172] fc7 does not need backward computation.
I0802 13:11:41.775898  7612 net.cpp:172] local6 does not need backward computation.
I0802 13:11:41.775949  7612 net.cpp:172] local5 does not need backward computation.
I0802 13:11:41.776000  7612 net.cpp:172] local4 does not need backward computation.
I0802 13:11:41.776046  7612 net.cpp:172] conv3 does not need backward computation.
I0802 13:11:41.776104  7612 net.cpp:172] pool2 does not need backward computation.
I0802 13:11:41.776160  7612 net.cpp:172] conv1 does not need backward computation.
I0802 13:11:41.776211  7612 net.cpp:208] This network produces output argmax
I0802 13:11:41.776361  7612 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 13:11:41.776480  7612 net.cpp:219] Network initialization done.
I0802 13:11:41.776532  7612 net.cpp:220] Memory required for data: 3759132
I0802 13:11:45.573439  7612 img-client.cpp:139] Reading input/face/cameron-diaz-11-152.jpg
I0802 13:11:45.577222  7612 img-client.cpp:139] Reading input/face/angelina-jolie-03-152.jpg
I0802 13:11:45.578963  7612 img-client.cpp:139] Reading input/face/daniel-craig-20-152.jpg
I0802 13:11:45.580296  7612 img-client.cpp:161] Aligning: input/face/cameron-diaz-11-152.jpg
I0802 13:11:45.742586  7612 img-client.cpp:161] Aligning: input/face/angelina-jolie-03-152.jpg
I0802 13:11:45.887126  7612 img-client.cpp:161] Aligning: input/face/daniel-craig-20-152.jpg
I0802 13:11:46.004945  7612 tonic.cpp:31] Reshaping input to dims: 3 3 152 152
I0802 13:11:46.006407  7612 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 13:31:00.352483  7612 img-client.cpp:222] Image: input/face/cameron-diaz-11-152.jpg class: 11
I0802 13:31:00.359900  7612 img-client.cpp:222] Image: input/face/angelina-jolie-03-152.jpg class: 0
I0802 13:31:00.360384  7612 img-client.cpp:222] Image: input/face/daniel-craig-20-152.jpg class: 0
684.79user 4.86system 19:20.24elapsed 59%CPU (0avgtext+0avgdata 3593472maxresident)k
0inputs+8outputs (0major+342261minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
img-face
********* Setting up benchmark img
********* Running benchmark as ./tonic-img-face
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 13:31:11.930214  6634 net.cpp:39] Initializing net from parameters: 
name: "face"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 1
  }
}
layers {
  bottom: "conv1"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  convolution_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "conv3"
  top: "local4"
  name: "local4"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 9
    stride: 1
  }
}
layers {
  bottom: "local4"
  top: "local5"
  name: "local5"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 7
    stride: 2
  }
}
layers {
  bottom: "local5"
  top: "local6"
  name: "local6"
  type: LOCAL
  local_param {
    num_output: 16
    kernel_size: 5
    stride: 1
  }
}
layers {
  bottom: "local6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 4096
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 83
  }
}
layers {
  bottom: "fc8"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  top: "argmax"
  name: "argmax"
  type: ARGMAX
}
input: "data"
input_dim: 1
input_dim: 3
input_dim: 152
input_dim: 152
I0802 13:31:11.931785  6634 net.cpp:358] Input 0 -> data
I0802 13:31:11.932219  6634 net.cpp:67] Creating Layer conv1
I0802 13:31:11.932312  6634 net.cpp:394] conv1 <- data
I0802 13:31:11.932390  6634 net.cpp:356] conv1 -> conv1
I0802 13:31:11.932570  6634 net.cpp:96] Setting up conv1
I0802 13:31:11.933071  6634 net.cpp:103] Top shape: 1 32 142 142 (645248)
I0802 13:31:11.933320  6634 net.cpp:67] Creating Layer pool2
I0802 13:31:11.933368  6634 net.cpp:394] pool2 <- conv1
I0802 13:31:11.933431  6634 net.cpp:356] pool2 -> pool2
I0802 13:31:11.933507  6634 net.cpp:96] Setting up pool2
I0802 13:31:11.933655  6634 net.cpp:103] Top shape: 1 32 71 71 (161312)
I0802 13:31:11.933711  6634 net.cpp:67] Creating Layer conv3
I0802 13:31:11.933769  6634 net.cpp:394] conv3 <- pool2
I0802 13:31:11.933822  6634 net.cpp:356] conv3 -> conv3
I0802 13:31:11.933882  6634 net.cpp:96] Setting up conv3
I0802 13:31:11.934182  6634 net.cpp:103] Top shape: 1 16 63 63 (63504)
I0802 13:31:11.934273  6634 net.cpp:67] Creating Layer local4
I0802 13:31:11.934320  6634 net.cpp:394] local4 <- conv3
I0802 13:31:11.934372  6634 net.cpp:356] local4 -> local4
I0802 13:31:11.934453  6634 net.cpp:96] Setting up local4
I0802 13:31:12.725363  6634 net.cpp:103] Top shape: 1 16 55 55 (48400)
I0802 13:31:12.730433  6634 net.cpp:67] Creating Layer local5
I0802 13:31:12.731524  6634 net.cpp:394] local5 <- local4
I0802 13:31:12.731875  6634 net.cpp:356] local5 -> local5
I0802 13:31:12.732007  6634 net.cpp:96] Setting up local5
I0802 13:31:12.835988  6634 net.cpp:103] Top shape: 1 16 25 25 (10000)
I0802 13:31:12.836767  6634 net.cpp:67] Creating Layer local6
I0802 13:31:12.836843  6634 net.cpp:394] local6 <- local5
I0802 13:31:12.836951  6634 net.cpp:356] local6 -> local6
I0802 13:31:12.837054  6634 net.cpp:96] Setting up local6
I0802 13:31:12.857084  6634 net.cpp:103] Top shape: 1 16 21 21 (7056)
I0802 13:31:12.857995  6634 net.cpp:67] Creating Layer fc7
I0802 13:31:12.858069  6634 net.cpp:394] fc7 <- local6
I0802 13:31:12.858166  6634 net.cpp:356] fc7 -> fc7
I0802 13:31:12.858338  6634 net.cpp:96] Setting up fc7
I0802 13:31:13.193629  6634 net.cpp:103] Top shape: 1 4096 1 1 (4096)
I0802 13:31:13.194385  6634 net.cpp:67] Creating Layer fc8
I0802 13:31:13.194443  6634 net.cpp:394] fc8 <- fc7
I0802 13:31:13.194520  6634 net.cpp:356] fc8 -> fc8
I0802 13:31:13.194664  6634 net.cpp:96] Setting up fc8
I0802 13:31:13.196168  6634 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 13:31:13.196796  6634 net.cpp:67] Creating Layer prob
I0802 13:31:13.196856  6634 net.cpp:394] prob <- fc8
I0802 13:31:13.196923  6634 net.cpp:356] prob -> prob
I0802 13:31:13.197067  6634 net.cpp:96] Setting up prob
I0802 13:31:13.197180  6634 net.cpp:103] Top shape: 1 83 1 1 (83)
I0802 13:31:13.197237  6634 net.cpp:67] Creating Layer argmax
I0802 13:31:13.197294  6634 net.cpp:394] argmax <- prob
I0802 13:31:13.197365  6634 net.cpp:356] argmax -> argmax
I0802 13:31:13.197418  6634 net.cpp:96] Setting up argmax
I0802 13:31:13.197499  6634 net.cpp:103] Top shape: 1 1 1 1 (1)
I0802 13:31:13.197545  6634 net.cpp:172] argmax does not need backward computation.
I0802 13:31:13.197646  6634 net.cpp:172] prob does not need backward computation.
I0802 13:31:13.197717  6634 net.cpp:172] fc8 does not need backward computation.
I0802 13:31:13.197767  6634 net.cpp:172] fc7 does not need backward computation.
I0802 13:31:13.197821  6634 net.cpp:172] local6 does not need backward computation.
I0802 13:31:13.197875  6634 net.cpp:172] local5 does not need backward computation.
I0802 13:31:13.197927  6634 net.cpp:172] local4 does not need backward computation.
I0802 13:31:13.197984  6634 net.cpp:172] conv3 does not need backward computation.
I0802 13:31:13.198040  6634 net.cpp:172] pool2 does not need backward computation.
I0802 13:31:13.198098  6634 net.cpp:172] conv1 does not need backward computation.
I0802 13:31:13.198151  6634 net.cpp:208] This network produces output argmax
I0802 13:31:13.198298  6634 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 13:31:13.198418  6634 net.cpp:219] Network initialization done.
I0802 13:31:13.198463  6634 net.cpp:220] Memory required for data: 3759132
I0802 13:31:16.931972  6634 img-client.cpp:139] Reading input/face/cameron-diaz-11-152.jpg
I0802 13:31:16.935425  6634 img-client.cpp:139] Reading input/face/angelina-jolie-03-152.jpg
I0802 13:31:16.937091  6634 img-client.cpp:139] Reading input/face/daniel-craig-20-152.jpg
I0802 13:31:16.938063  6634 img-client.cpp:161] Aligning: input/face/cameron-diaz-11-152.jpg
I0802 13:31:17.066891  6634 img-client.cpp:161] Aligning: input/face/angelina-jolie-03-152.jpg
I0802 13:31:17.181226  6634 img-client.cpp:161] Aligning: input/face/daniel-craig-20-152.jpg
I0802 13:31:17.317641  6634 tonic.cpp:31] Reshaping input to dims: 3 3 152 152
I0802 13:31:17.318413  6634 tonic.cpp:37] Reshaping output to dims: 3 1 1 1
I0802 13:50:19.785871  6634 img-client.cpp:222] Image: input/face/cameron-diaz-11-152.jpg class: 11
I0802 13:50:19.794168  6634 img-client.cpp:222] Image: input/face/angelina-jolie-03-152.jpg class: 0
I0802 13:50:19.794544  6634 img-client.cpp:222] Image: input/face/daniel-craig-20-152.jpg class: 0
679.94user 4.52system 19:08.19elapsed 59%CPU (0avgtext+0avgdata 3593456maxresident)k
0inputs+8outputs (0major+342261minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-pos
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-pos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 13:50:31.895247  6059 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 13:50:31.901782  6059 net.cpp:358] Input 0 -> data
I0802 13:50:31.902158  6059 net.cpp:67] Creating Layer fc1
I0802 13:50:31.902238  6059 net.cpp:394] fc1 <- data
I0802 13:50:31.902318  6059 net.cpp:356] fc1 -> fc1
I0802 13:50:31.902483  6059 net.cpp:96] Setting up fc1
I0802 13:50:31.903174  6059 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:50:31.903448  6059 net.cpp:67] Creating Layer htanh
I0802 13:50:31.903518  6059 net.cpp:394] htanh <- fc1
I0802 13:50:31.903556  6059 net.cpp:356] htanh -> htanh
I0802 13:50:31.903838  6059 net.cpp:96] Setting up htanh
I0802 13:50:31.903945  6059 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:50:31.904012  6059 net.cpp:67] Creating Layer fc3
I0802 13:50:31.904064  6059 net.cpp:394] fc3 <- htanh
I0802 13:50:31.904137  6059 net.cpp:356] fc3 -> fc3
I0802 13:50:31.904201  6059 net.cpp:96] Setting up fc3
I0802 13:50:31.904347  6059 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 13:50:31.904412  6059 net.cpp:172] fc3 does not need backward computation.
I0802 13:50:31.904495  6059 net.cpp:172] htanh does not need backward computation.
I0802 13:50:31.904566  6059 net.cpp:172] fc1 does not need backward computation.
I0802 13:50:31.904618  6059 net.cpp:208] This network produces output fc3
I0802 13:50:31.904718  6059 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 13:50:31.906370  6059 net.cpp:219] Network initialization done.
I0802 13:50:31.906479  6059 net.cpp:220] Memory required for data: 2580
I0802 13:50:31.930220  6059 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 13:50:31.931520  6059 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
---------------------------------------------------------
colocating with libquantum
nlp-pos
Command terminated by signal 9
128.61user 19.20system 4:11.49elapsed 58%CPU (0avgtext+0avgdata 15531984maxresident)k
1505960inputs+0outputs (5597major+1220335minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-pos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 13:54:44.783757 23441 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 13:54:44.785275 23441 net.cpp:358] Input 0 -> data
I0802 13:54:44.785570 23441 net.cpp:67] Creating Layer fc1
I0802 13:54:44.785641 23441 net.cpp:394] fc1 <- data
I0802 13:54:44.785696 23441 net.cpp:356] fc1 -> fc1
I0802 13:54:44.785831 23441 net.cpp:96] Setting up fc1
I0802 13:54:44.788105 23441 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:54:44.789574 23441 net.cpp:67] Creating Layer htanh
I0802 13:54:44.789660 23441 net.cpp:394] htanh <- fc1
I0802 13:54:44.789721 23441 net.cpp:356] htanh -> htanh
I0802 13:54:44.789929 23441 net.cpp:96] Setting up htanh
I0802 13:54:44.789997 23441 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:54:44.790055 23441 net.cpp:67] Creating Layer fc3
I0802 13:54:44.790101 23441 net.cpp:394] fc3 <- htanh
I0802 13:54:44.790166 23441 net.cpp:356] fc3 -> fc3
I0802 13:54:44.790251 23441 net.cpp:96] Setting up fc3
I0802 13:54:44.790472 23441 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 13:54:44.790539 23441 net.cpp:172] fc3 does not need backward computation.
I0802 13:54:44.790594 23441 net.cpp:172] htanh does not need backward computation.
I0802 13:54:44.790657 23441 net.cpp:172] fc1 does not need backward computation.
I0802 13:54:44.790705 23441 net.cpp:208] This network produces output fc3
I0802 13:54:44.790786 23441 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 13:54:44.790858 23441 net.cpp:219] Network initialization done.
I0802 13:54:44.790917 23441 net.cpp:220] Memory required for data: 2580
I0802 13:54:44.821718 23441 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 13:54:44.825242 23441 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
Command terminated by signal 9
125.98user 10.75system 3:59.31elapsed 57%CPU (0avgtext+0avgdata 15462288maxresident)k
260576inputs+0outputs (383major+1216035minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-pos
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-pos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 13:58:54.340507  8533 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 13:58:54.343297  8533 net.cpp:358] Input 0 -> data
I0802 13:58:54.346217  8533 net.cpp:67] Creating Layer fc1
I0802 13:58:54.347980  8533 net.cpp:394] fc1 <- data
I0802 13:58:54.348400  8533 net.cpp:356] fc1 -> fc1
I0802 13:58:54.348693  8533 net.cpp:96] Setting up fc1
I0802 13:58:54.353198  8533 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:58:54.354989  8533 net.cpp:67] Creating Layer htanh
I0802 13:58:54.355101  8533 net.cpp:394] htanh <- fc1
I0802 13:58:54.355165  8533 net.cpp:356] htanh -> htanh
I0802 13:58:54.355274  8533 net.cpp:96] Setting up htanh
I0802 13:58:54.355342  8533 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 13:58:54.355401  8533 net.cpp:67] Creating Layer fc3
I0802 13:58:54.355468  8533 net.cpp:394] fc3 <- htanh
I0802 13:58:54.355521  8533 net.cpp:356] fc3 -> fc3
I0802 13:58:54.355665  8533 net.cpp:96] Setting up fc3
I0802 13:58:54.355943  8533 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 13:58:54.359200  8533 net.cpp:172] fc3 does not need backward computation.
I0802 13:58:54.359781  8533 net.cpp:172] htanh does not need backward computation.
I0802 13:58:54.359848  8533 net.cpp:172] fc1 does not need backward computation.
I0802 13:58:54.359886  8533 net.cpp:208] This network produces output fc3
I0802 13:58:54.360013  8533 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 13:58:54.360088  8533 net.cpp:219] Network initialization done.
I0802 13:58:54.360133  8533 net.cpp:220] Memory required for data: 2580
I0802 13:58:54.384881  8533 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 13:58:54.385445  8533 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
---------------------------------------------------------
colocating with libquantum
nlp-pos
Command terminated by signal 9
126.11user 11.12system 3:59.14elapsed 57%CPU (0avgtext+0avgdata 15522160maxresident)k
181976inputs+0outputs (62major+1219457minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-pos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:03:03.608028 25913 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:03:03.614140 25913 net.cpp:358] Input 0 -> data
I0802 14:03:03.614485 25913 net.cpp:67] Creating Layer fc1
I0802 14:03:03.614565 25913 net.cpp:394] fc1 <- data
I0802 14:03:03.614662 25913 net.cpp:356] fc1 -> fc1
I0802 14:03:03.614802 25913 net.cpp:96] Setting up fc1
I0802 14:03:03.617115 25913 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:03:03.620425 25913 net.cpp:67] Creating Layer htanh
I0802 14:03:03.620645 25913 net.cpp:394] htanh <- fc1
I0802 14:03:03.620699 25913 net.cpp:356] htanh -> htanh
I0802 14:03:03.620857 25913 net.cpp:96] Setting up htanh
I0802 14:03:03.620936 25913 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:03:03.620990 25913 net.cpp:67] Creating Layer fc3
I0802 14:03:03.621058 25913 net.cpp:394] fc3 <- htanh
I0802 14:03:03.621114 25913 net.cpp:356] fc3 -> fc3
I0802 14:03:03.621193 25913 net.cpp:96] Setting up fc3
I0802 14:03:03.621428 25913 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:03:03.621495 25913 net.cpp:172] fc3 does not need backward computation.
I0802 14:03:03.621548 25913 net.cpp:172] htanh does not need backward computation.
I0802 14:03:03.621606 25913 net.cpp:172] fc1 does not need backward computation.
I0802 14:03:03.621654 25913 net.cpp:208] This network produces output fc3
I0802 14:03:03.621732 25913 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:03:03.621783 25913 net.cpp:219] Network initialization done.
I0802 14:03:03.621850 25913 net.cpp:220] Memory required for data: 2580
I0802 14:03:03.654357 25913 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:03:03.663913 25913 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
---------------------------------------------------------
colocating with libquantum
nlp-pos
Command terminated by signal 9
126.19user 10.37system 3:56.86elapsed 57%CPU (0avgtext+0avgdata 15480224maxresident)k
219592inputs+0outputs (219major+1217038minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-pos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:07:10.408818 10894 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:07:10.414238 10894 net.cpp:358] Input 0 -> data
I0802 14:07:10.414594 10894 net.cpp:67] Creating Layer fc1
I0802 14:07:10.414664 10894 net.cpp:394] fc1 <- data
I0802 14:07:10.414767 10894 net.cpp:356] fc1 -> fc1
I0802 14:07:10.414904 10894 net.cpp:96] Setting up fc1
I0802 14:07:10.417767 10894 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:07:10.420281 10894 net.cpp:67] Creating Layer htanh
I0802 14:07:10.421758 10894 net.cpp:394] htanh <- fc1
I0802 14:07:10.421982 10894 net.cpp:356] htanh -> htanh
I0802 14:07:10.422195 10894 net.cpp:96] Setting up htanh
I0802 14:07:10.422307 10894 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:07:10.422379 10894 net.cpp:67] Creating Layer fc3
I0802 14:07:10.422435 10894 net.cpp:394] fc3 <- htanh
I0802 14:07:10.422485 10894 net.cpp:356] fc3 -> fc3
I0802 14:07:10.422557 10894 net.cpp:96] Setting up fc3
I0802 14:07:10.422741 10894 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:07:10.425732 10894 net.cpp:172] fc3 does not need backward computation.
I0802 14:07:10.426162 10894 net.cpp:172] htanh does not need backward computation.
I0802 14:07:10.426213 10894 net.cpp:172] fc1 does not need backward computation.
I0802 14:07:10.426367 10894 net.cpp:208] This network produces output fc3
I0802 14:07:10.426489 10894 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:07:10.426548 10894 net.cpp:219] Network initialization done.
I0802 14:07:10.426609 10894 net.cpp:220] Memory required for data: 2580
I0802 14:07:10.451964 10894 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:07:10.452993 10894 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
Command terminated by signal 9
127.11user 10.42system 4:00.86elapsed 57%CPU (0avgtext+0avgdata 15484544maxresident)k
191752inputs+0outputs (104major+1217204minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-chk
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-chk
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:11:21.756305 28581 net.cpp:39] Initializing net from parameters: 
name: "chk"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 42
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:11:21.760185 28581 net.cpp:358] Input 0 -> data
I0802 14:11:21.760499 28581 net.cpp:67] Creating Layer fc1
I0802 14:11:21.761247 28581 net.cpp:394] fc1 <- data
I0802 14:11:21.761339 28581 net.cpp:356] fc1 -> fc1
I0802 14:11:21.761467 28581 net.cpp:96] Setting up fc1
I0802 14:11:21.765162 28581 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:11:21.766868 28581 net.cpp:67] Creating Layer htanh
I0802 14:11:21.766969 28581 net.cpp:394] htanh <- fc1
I0802 14:11:21.767031 28581 net.cpp:356] htanh -> htanh
I0802 14:11:21.767134 28581 net.cpp:96] Setting up htanh
I0802 14:11:21.767200 28581 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:11:21.767261 28581 net.cpp:67] Creating Layer fc3
I0802 14:11:21.767321 28581 net.cpp:394] fc3 <- htanh
I0802 14:11:21.767393 28581 net.cpp:356] fc3 -> fc3
I0802 14:11:21.768120 28581 net.cpp:96] Setting up fc3
I0802 14:11:21.768323 28581 net.cpp:103] Top shape: 1 42 1 1 (42)
I0802 14:11:21.768394 28581 net.cpp:172] fc3 does not need backward computation.
I0802 14:11:21.768558 28581 net.cpp:172] htanh does not need backward computation.
I0802 14:11:21.768599 28581 net.cpp:172] fc1 does not need backward computation.
I0802 14:11:21.768652 28581 net.cpp:208] This network produces output fc3
I0802 14:11:21.768734 28581 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:11:21.768790 28581 net.cpp:219] Network initialization done.
I0802 14:11:21.768836 28581 net.cpp:220] Memory required for data: 2568
I0802 14:11:21.807092 28581 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:11:21.812892 28581 net.cpp:358] Input 0 -> data
I0802 14:11:21.813329 28581 net.cpp:67] Creating Layer fc1
I0802 14:11:21.813390 28581 net.cpp:394] fc1 <- data
I0802 14:11:21.813897 28581 net.cpp:356] fc1 -> fc1
I0802 14:11:21.813982 28581 net.cpp:96] Setting up fc1
I0802 14:11:21.820201 28581 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:11:21.824082 28581 net.cpp:67] Creating Layer htanh
I0802 14:11:21.824494 28581 net.cpp:394] htanh <- fc1
I0802 14:11:21.824565 28581 net.cpp:356] htanh -> htanh
I0802 14:11:21.824753 28581 net.cpp:96] Setting up htanh
I0802 14:11:21.824817 28581 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:11:21.824892 28581 net.cpp:67] Creating Layer fc3
I0802 14:11:21.824939 28581 net.cpp:394] fc3 <- htanh
I0802 14:11:21.825004 28581 net.cpp:356] fc3 -> fc3
I0802 14:11:21.825060 28581 net.cpp:96] Setting up fc3
I0802 14:11:21.825186 28581 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:11:21.825261 28581 net.cpp:172] fc3 does not need backward computation.
I0802 14:11:21.825315 28581 net.cpp:172] htanh does not need backward computation.
I0802 14:11:21.825356 28581 net.cpp:172] fc1 does not need backward computation.
I0802 14:11:21.825407 28581 net.cpp:208] This network produces output fc3
I0802 14:11:21.825453 28581 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:11:21.825510 28581 net.cpp:219] Network initialization done.
I0802 14:11:21.825549 28581 net.cpp:220] Memory required for data: 2580
I0802 14:11:21.843443 28581 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:11:21.848201 28581 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
I0802 14:11:21.853909 28581 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:11:21.854388 28581 tonic.cpp:37] Reshaping output to dims: 28 42 1 1
---------------------------------------------------------
colocating with libquantum
nlp-chk
Command terminated by signal 9
117.74user 10.92system 3:45.23elapsed 57%CPU (0avgtext+0avgdata 15468064maxresident)k
261672inputs+0outputs (380major+1216389minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-chk
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:15:17.192502 12552 net.cpp:39] Initializing net from parameters: 
name: "chk"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 42
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:15:17.200933 12552 net.cpp:358] Input 0 -> data
I0802 14:15:17.201341 12552 net.cpp:67] Creating Layer fc1
I0802 14:15:17.201408 12552 net.cpp:394] fc1 <- data
I0802 14:15:17.201477 12552 net.cpp:356] fc1 -> fc1
I0802 14:15:17.201633 12552 net.cpp:96] Setting up fc1
I0802 14:15:17.203559 12552 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:15:17.205132 12552 net.cpp:67] Creating Layer htanh
I0802 14:15:17.205220 12552 net.cpp:394] htanh <- fc1
I0802 14:15:17.205292 12552 net.cpp:356] htanh -> htanh
I0802 14:15:17.205445 12552 net.cpp:96] Setting up htanh
I0802 14:15:17.205476 12552 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:15:17.205494 12552 net.cpp:67] Creating Layer fc3
I0802 14:15:17.205499 12552 net.cpp:394] fc3 <- htanh
I0802 14:15:17.205508 12552 net.cpp:356] fc3 -> fc3
I0802 14:15:17.205519 12552 net.cpp:96] Setting up fc3
I0802 14:15:17.205683 12552 net.cpp:103] Top shape: 1 42 1 1 (42)
I0802 14:15:17.205698 12552 net.cpp:172] fc3 does not need backward computation.
I0802 14:15:17.205708 12552 net.cpp:172] htanh does not need backward computation.
I0802 14:15:17.205711 12552 net.cpp:172] fc1 does not need backward computation.
I0802 14:15:17.205716 12552 net.cpp:208] This network produces output fc3
I0802 14:15:17.205739 12552 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:15:17.205754 12552 net.cpp:219] Network initialization done.
I0802 14:15:17.205757 12552 net.cpp:220] Memory required for data: 2568
I0802 14:15:17.229631 12552 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:15:17.230170 12552 net.cpp:358] Input 0 -> data
I0802 14:15:17.230365 12552 net.cpp:67] Creating Layer fc1
I0802 14:15:17.230417 12552 net.cpp:394] fc1 <- data
I0802 14:15:17.230470 12552 net.cpp:356] fc1 -> fc1
I0802 14:15:17.230550 12552 net.cpp:96] Setting up fc1
I0802 14:15:17.231142 12552 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:15:17.231278 12552 net.cpp:67] Creating Layer htanh
I0802 14:15:17.231328 12552 net.cpp:394] htanh <- fc1
I0802 14:15:17.231402 12552 net.cpp:356] htanh -> htanh
I0802 14:15:17.231464 12552 net.cpp:96] Setting up htanh
I0802 14:15:17.231536 12552 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:15:17.231696 12552 net.cpp:67] Creating Layer fc3
I0802 14:15:17.231757 12552 net.cpp:394] fc3 <- htanh
I0802 14:15:17.231812 12552 net.cpp:356] fc3 -> fc3
I0802 14:15:17.231907 12552 net.cpp:96] Setting up fc3
I0802 14:15:17.232053 12552 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:15:17.232118 12552 net.cpp:172] fc3 does not need backward computation.
I0802 14:15:17.232166 12552 net.cpp:172] htanh does not need backward computation.
I0802 14:15:17.232218 12552 net.cpp:172] fc1 does not need backward computation.
I0802 14:15:17.232259 12552 net.cpp:208] This network produces output fc3
I0802 14:15:17.232322 12552 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:15:17.232370 12552 net.cpp:219] Network initialization done.
I0802 14:15:17.232445 12552 net.cpp:220] Memory required for data: 2580
I0802 14:15:17.242611 12552 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:15:17.243161 12552 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
I0802 14:15:17.247992 12552 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:15:17.248467 12552 tonic.cpp:37] Reshaping output to dims: 28 42 1 1
---------------------------------------------------------
colocating with libquantum
nlp-chk
Command terminated by signal 9
118.46user 11.26system 3:46.55elapsed 57%CPU (0avgtext+0avgdata 15512400maxresident)k
945096inputs+0outputs (3260major+1220036minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-chk
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:19:11.666893 28886 net.cpp:39] Initializing net from parameters: 
name: "chk"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 42
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:19:11.671020 28886 net.cpp:358] Input 0 -> data
I0802 14:19:11.673913 28886 net.cpp:67] Creating Layer fc1
I0802 14:19:11.674316 28886 net.cpp:394] fc1 <- data
I0802 14:19:11.674386 28886 net.cpp:356] fc1 -> fc1
I0802 14:19:11.674655 28886 net.cpp:96] Setting up fc1
I0802 14:19:11.677153 28886 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:19:11.679689 28886 net.cpp:67] Creating Layer htanh
I0802 14:19:11.682972 28886 net.cpp:394] htanh <- fc1
I0802 14:19:11.683465 28886 net.cpp:356] htanh -> htanh
I0802 14:19:11.683785 28886 net.cpp:96] Setting up htanh
I0802 14:19:11.683903 28886 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:19:11.683974 28886 net.cpp:67] Creating Layer fc3
I0802 14:19:11.684041 28886 net.cpp:394] fc3 <- htanh
I0802 14:19:11.684094 28886 net.cpp:356] fc3 -> fc3
I0802 14:19:11.684183 28886 net.cpp:96] Setting up fc3
I0802 14:19:11.684425 28886 net.cpp:103] Top shape: 1 42 1 1 (42)
I0802 14:19:11.684514 28886 net.cpp:172] fc3 does not need backward computation.
I0802 14:19:11.684567 28886 net.cpp:172] htanh does not need backward computation.
I0802 14:19:11.684653 28886 net.cpp:172] fc1 does not need backward computation.
I0802 14:19:11.684698 28886 net.cpp:208] This network produces output fc3
I0802 14:19:11.684789 28886 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:19:11.684850 28886 net.cpp:219] Network initialization done.
I0802 14:19:11.684921 28886 net.cpp:220] Memory required for data: 2568
I0802 14:19:11.720199 28886 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:19:11.724074 28886 net.cpp:358] Input 0 -> data
I0802 14:19:11.724613 28886 net.cpp:67] Creating Layer fc1
I0802 14:19:11.724812 28886 net.cpp:394] fc1 <- data
I0802 14:19:11.724867 28886 net.cpp:356] fc1 -> fc1
I0802 14:19:11.725042 28886 net.cpp:96] Setting up fc1
I0802 14:19:11.725625 28886 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:19:11.725749 28886 net.cpp:67] Creating Layer htanh
I0802 14:19:11.725801 28886 net.cpp:394] htanh <- fc1
I0802 14:19:11.725875 28886 net.cpp:356] htanh -> htanh
I0802 14:19:11.725934 28886 net.cpp:96] Setting up htanh
I0802 14:19:11.725996 28886 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:19:11.726048 28886 net.cpp:67] Creating Layer fc3
I0802 14:19:11.726106 28886 net.cpp:394] fc3 <- htanh
I0802 14:19:11.726155 28886 net.cpp:356] fc3 -> fc3
I0802 14:19:11.726227 28886 net.cpp:96] Setting up fc3
I0802 14:19:11.726348 28886 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:19:11.726405 28886 net.cpp:172] fc3 does not need backward computation.
I0802 14:19:11.726454 28886 net.cpp:172] htanh does not need backward computation.
I0802 14:19:11.726524 28886 net.cpp:172] fc1 does not need backward computation.
I0802 14:19:11.726572 28886 net.cpp:208] This network produces output fc3
I0802 14:19:11.726635 28886 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:19:11.726687 28886 net.cpp:219] Network initialization done.
I0802 14:19:11.726744 28886 net.cpp:220] Memory required for data: 2580
I0802 14:19:11.757725 28886 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:19:11.758319 28886 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
I0802 14:19:11.768254 28886 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:19:11.768782 28886 tonic.cpp:37] Reshaping output to dims: 28 42 1 1
---------------------------------------------------------
colocating with libquantum
nlp-chk
Command terminated by signal 9
118.03user 10.73system 3:46.81elapsed 56%CPU (0avgtext+0avgdata 15495248maxresident)k
184976inputs+0outputs (67major+1217810minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-chk
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:23:08.599040 12757 net.cpp:39] Initializing net from parameters: 
name: "chk"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 42
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:23:08.601531 12757 net.cpp:358] Input 0 -> data
I0802 14:23:08.603914 12757 net.cpp:67] Creating Layer fc1
I0802 14:23:08.604277 12757 net.cpp:394] fc1 <- data
I0802 14:23:08.604349 12757 net.cpp:356] fc1 -> fc1
I0802 14:23:08.604550 12757 net.cpp:96] Setting up fc1
I0802 14:23:08.606808 12757 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:23:08.609246 12757 net.cpp:67] Creating Layer htanh
I0802 14:23:08.612107 12757 net.cpp:394] htanh <- fc1
I0802 14:23:08.612587 12757 net.cpp:356] htanh -> htanh
I0802 14:23:08.612854 12757 net.cpp:96] Setting up htanh
I0802 14:23:08.612988 12757 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:23:08.613101 12757 net.cpp:67] Creating Layer fc3
I0802 14:23:08.613158 12757 net.cpp:394] fc3 <- htanh
I0802 14:23:08.613212 12757 net.cpp:356] fc3 -> fc3
I0802 14:23:08.613312 12757 net.cpp:96] Setting up fc3
I0802 14:23:08.613503 12757 net.cpp:103] Top shape: 1 42 1 1 (42)
I0802 14:23:08.613592 12757 net.cpp:172] fc3 does not need backward computation.
I0802 14:23:08.613656 12757 net.cpp:172] htanh does not need backward computation.
I0802 14:23:08.613731 12757 net.cpp:172] fc1 does not need backward computation.
I0802 14:23:08.613778 12757 net.cpp:208] This network produces output fc3
I0802 14:23:08.613873 12757 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:23:08.613941 12757 net.cpp:219] Network initialization done.
I0802 14:23:08.614007 12757 net.cpp:220] Memory required for data: 2568
I0802 14:23:08.651253 12757 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:23:08.660034 12757 net.cpp:358] Input 0 -> data
I0802 14:23:08.660531 12757 net.cpp:67] Creating Layer fc1
I0802 14:23:08.660722 12757 net.cpp:394] fc1 <- data
I0802 14:23:08.660786 12757 net.cpp:356] fc1 -> fc1
I0802 14:23:08.660892 12757 net.cpp:96] Setting up fc1
I0802 14:23:08.661499 12757 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:23:08.661649 12757 net.cpp:67] Creating Layer htanh
I0802 14:23:08.661701 12757 net.cpp:394] htanh <- fc1
I0802 14:23:08.661774 12757 net.cpp:356] htanh -> htanh
I0802 14:23:08.661839 12757 net.cpp:96] Setting up htanh
I0802 14:23:08.661911 12757 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:23:08.661965 12757 net.cpp:67] Creating Layer fc3
I0802 14:23:08.662030 12757 net.cpp:394] fc3 <- htanh
I0802 14:23:08.662094 12757 net.cpp:356] fc3 -> fc3
I0802 14:23:08.662169 12757 net.cpp:96] Setting up fc3
I0802 14:23:08.662283 12757 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:23:08.662348 12757 net.cpp:172] fc3 does not need backward computation.
I0802 14:23:08.662395 12757 net.cpp:172] htanh does not need backward computation.
I0802 14:23:08.662446 12757 net.cpp:172] fc1 does not need backward computation.
I0802 14:23:08.662488 12757 net.cpp:208] This network produces output fc3
I0802 14:23:08.662575 12757 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:23:08.662636 12757 net.cpp:219] Network initialization done.
I0802 14:23:08.662694 12757 net.cpp:220] Memory required for data: 2580
I0802 14:23:08.679431 12757 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:23:08.682808 12757 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
I0802 14:23:08.690035 12757 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:23:08.690641 12757 tonic.cpp:37] Reshaping output to dims: 28 42 1 1
Command terminated by signal 9
115.86user 10.28system 3:40.31elapsed 57%CPU (0avgtext+0avgdata 15413760maxresident)k
203112inputs+0outputs (145major+1212832minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-chk
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-chk
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:27:00.102126 28764 net.cpp:39] Initializing net from parameters: 
name: "chk"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 42
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:27:00.103508 28764 net.cpp:358] Input 0 -> data
I0802 14:27:00.106123 28764 net.cpp:67] Creating Layer fc1
I0802 14:27:00.106534 28764 net.cpp:394] fc1 <- data
I0802 14:27:00.106607 28764 net.cpp:356] fc1 -> fc1
I0802 14:27:00.106863 28764 net.cpp:96] Setting up fc1
I0802 14:27:00.109325 28764 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:27:00.110052 28764 net.cpp:67] Creating Layer htanh
I0802 14:27:00.110121 28764 net.cpp:394] htanh <- fc1
I0802 14:27:00.110306 28764 net.cpp:356] htanh -> htanh
I0802 14:27:00.110389 28764 net.cpp:96] Setting up htanh
I0802 14:27:00.110455 28764 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:27:00.111032 28764 net.cpp:67] Creating Layer fc3
I0802 14:27:00.111099 28764 net.cpp:394] fc3 <- htanh
I0802 14:27:00.111140 28764 net.cpp:356] fc3 -> fc3
I0802 14:27:00.111188 28764 net.cpp:96] Setting up fc3
I0802 14:27:00.111362 28764 net.cpp:103] Top shape: 1 42 1 1 (42)
I0802 14:27:00.111431 28764 net.cpp:172] fc3 does not need backward computation.
I0802 14:27:00.111485 28764 net.cpp:172] htanh does not need backward computation.
I0802 14:27:00.111523 28764 net.cpp:172] fc1 does not need backward computation.
I0802 14:27:00.111582 28764 net.cpp:208] This network produces output fc3
I0802 14:27:00.111814 28764 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:27:00.112434 28764 net.cpp:219] Network initialization done.
I0802 14:27:00.112517 28764 net.cpp:220] Memory required for data: 2568
I0802 14:27:00.174087 28764 net.cpp:39] Initializing net from parameters: 
name: "pos"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 45
  }
}
input: "data"
input_dim: 1
input_dim: 300
input_dim: 1
input_dim: 1
I0802 14:27:00.174273 28764 net.cpp:358] Input 0 -> data
I0802 14:27:00.174351 28764 net.cpp:67] Creating Layer fc1
I0802 14:27:00.174360 28764 net.cpp:394] fc1 <- data
I0802 14:27:00.174372 28764 net.cpp:356] fc1 -> fc1
I0802 14:27:00.174387 28764 net.cpp:96] Setting up fc1
I0802 14:27:00.174916 28764 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:27:00.174958 28764 net.cpp:67] Creating Layer htanh
I0802 14:27:00.174964 28764 net.cpp:394] htanh <- fc1
I0802 14:27:00.174975 28764 net.cpp:356] htanh -> htanh
I0802 14:27:00.174986 28764 net.cpp:96] Setting up htanh
I0802 14:27:00.174994 28764 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:27:00.175003 28764 net.cpp:67] Creating Layer fc3
I0802 14:27:00.175007 28764 net.cpp:394] fc3 <- htanh
I0802 14:27:00.175015 28764 net.cpp:356] fc3 -> fc3
I0802 14:27:00.175022 28764 net.cpp:96] Setting up fc3
I0802 14:27:00.175071 28764 net.cpp:103] Top shape: 1 45 1 1 (45)
I0802 14:27:00.175081 28764 net.cpp:172] fc3 does not need backward computation.
I0802 14:27:00.175087 28764 net.cpp:172] htanh does not need backward computation.
I0802 14:27:00.175091 28764 net.cpp:172] fc1 does not need backward computation.
I0802 14:27:00.175096 28764 net.cpp:208] This network produces output fc3
I0802 14:27:00.175108 28764 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:27:00.175115 28764 net.cpp:219] Network initialization done.
I0802 14:27:00.175119 28764 net.cpp:220] Memory required for data: 2580
I0802 14:27:00.194319 28764 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:27:00.194653 28764 tonic.cpp:37] Reshaping output to dims: 28 45 1 1
I0802 14:27:00.200615 28764 tonic.cpp:31] Reshaping input to dims: 28 300 1 1
I0802 14:27:00.201128 28764 tonic.cpp:37] Reshaping output to dims: 28 42 1 1
Command terminated by signal 9
116.45user 10.64system 3:44.18elapsed 56%CPU (0avgtext+0avgdata 15430128maxresident)k
213208inputs+0outputs (189major+1212012minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-ner
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-ner
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:30:55.352488 13042 net.cpp:39] Initializing net from parameters: 
name: "ner"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 17
  }
}
input: "data"
input_dim: 1
input_dim: 375
input_dim: 1
input_dim: 1
I0802 14:30:55.360687 13042 net.cpp:358] Input 0 -> data
I0802 14:30:55.360980 13042 net.cpp:67] Creating Layer fc1
I0802 14:30:55.361052 13042 net.cpp:394] fc1 <- data
I0802 14:30:55.361147 13042 net.cpp:356] fc1 -> fc1
I0802 14:30:55.361276 13042 net.cpp:96] Setting up fc1
I0802 14:30:55.368976 13042 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:30:55.370733 13042 net.cpp:67] Creating Layer htanh
I0802 14:30:55.370843 13042 net.cpp:394] htanh <- fc1
I0802 14:30:55.370910 13042 net.cpp:356] htanh -> htanh
I0802 14:30:55.371016 13042 net.cpp:96] Setting up htanh
I0802 14:30:55.371101 13042 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:30:55.371161 13042 net.cpp:67] Creating Layer fc3
I0802 14:30:55.371201 13042 net.cpp:394] fc3 <- htanh
I0802 14:30:55.371265 13042 net.cpp:356] fc3 -> fc3
I0802 14:30:55.371335 13042 net.cpp:96] Setting up fc3
I0802 14:30:55.371510 13042 net.cpp:103] Top shape: 1 17 1 1 (17)
I0802 14:30:55.371562 13042 net.cpp:172] fc3 does not need backward computation.
I0802 14:30:55.371728 13042 net.cpp:172] htanh does not need backward computation.
I0802 14:30:55.371778 13042 net.cpp:172] fc1 does not need backward computation.
I0802 14:30:55.371836 13042 net.cpp:208] This network produces output fc3
I0802 14:30:55.371911 13042 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:30:55.371969 13042 net.cpp:219] Network initialization done.
I0802 14:30:55.372014 13042 net.cpp:220] Memory required for data: 2468
I0802 14:30:55.404876 13042 tonic.cpp:31] Reshaping input to dims: 28 375 1 1
I0802 14:30:55.405433 13042 tonic.cpp:37] Reshaping output to dims: 28 17 1 1
---------------------------------------------------------
colocating with libquantum
nlp-ner
Command terminated by signal 9
92.42user 9.75system 2:56.66elapsed 57%CPU (0avgtext+0avgdata 15471584maxresident)k
187640inputs+0outputs (83major+1216394minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-ner
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:34:01.852867 26054 net.cpp:39] Initializing net from parameters: 
name: "ner"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 17
  }
}
input: "data"
input_dim: 1
input_dim: 375
input_dim: 1
input_dim: 1
I0802 14:34:01.857167 26054 net.cpp:358] Input 0 -> data
I0802 14:34:01.857519 26054 net.cpp:67] Creating Layer fc1
I0802 14:34:01.857595 26054 net.cpp:394] fc1 <- data
I0802 14:34:01.857661 26054 net.cpp:356] fc1 -> fc1
I0802 14:34:01.857795 26054 net.cpp:96] Setting up fc1
I0802 14:34:01.861659 26054 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:34:01.863365 26054 net.cpp:67] Creating Layer htanh
I0802 14:34:01.863468 26054 net.cpp:394] htanh <- fc1
I0802 14:34:01.864187 26054 net.cpp:356] htanh -> htanh
I0802 14:34:01.864292 26054 net.cpp:96] Setting up htanh
I0802 14:34:01.864383 26054 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:34:01.864465 26054 net.cpp:67] Creating Layer fc3
I0802 14:34:01.864506 26054 net.cpp:394] fc3 <- htanh
I0802 14:34:01.864573 26054 net.cpp:356] fc3 -> fc3
I0802 14:34:01.864646 26054 net.cpp:96] Setting up fc3
I0802 14:34:01.864776 26054 net.cpp:103] Top shape: 1 17 1 1 (17)
I0802 14:34:01.864852 26054 net.cpp:172] fc3 does not need backward computation.
I0802 14:34:01.864908 26054 net.cpp:172] htanh does not need backward computation.
I0802 14:34:01.864962 26054 net.cpp:172] fc1 does not need backward computation.
I0802 14:34:01.865030 26054 net.cpp:208] This network produces output fc3
I0802 14:34:01.865119 26054 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:34:01.865186 26054 net.cpp:219] Network initialization done.
I0802 14:34:01.865231 26054 net.cpp:220] Memory required for data: 2468
I0802 14:34:01.892688 26054 tonic.cpp:31] Reshaping input to dims: 28 375 1 1
I0802 14:34:01.893895 26054 tonic.cpp:37] Reshaping output to dims: 28 17 1 1
Command terminated by signal 9
91.72user 9.90system 2:57.15elapsed 57%CPU (0avgtext+0avgdata 15432704maxresident)k
182832inputs+0outputs (65major+1213854minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-ner
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-ner
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:37:10.471218  6271 net.cpp:39] Initializing net from parameters: 
name: "ner"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 17
  }
}
input: "data"
input_dim: 1
input_dim: 375
input_dim: 1
input_dim: 1
I0802 14:37:10.472528  6271 net.cpp:358] Input 0 -> data
I0802 14:37:10.472785  6271 net.cpp:67] Creating Layer fc1
I0802 14:37:10.472853  6271 net.cpp:394] fc1 <- data
I0802 14:37:10.472930  6271 net.cpp:356] fc1 -> fc1
I0802 14:37:10.473050  6271 net.cpp:96] Setting up fc1
I0802 14:37:10.475451  6271 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:37:10.477516  6271 net.cpp:67] Creating Layer htanh
I0802 14:37:10.477663  6271 net.cpp:394] htanh <- fc1
I0802 14:37:10.477730  6271 net.cpp:356] htanh -> htanh
I0802 14:37:10.477856  6271 net.cpp:96] Setting up htanh
I0802 14:37:10.477963  6271 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:37:10.478029  6271 net.cpp:67] Creating Layer fc3
I0802 14:37:10.478075  6271 net.cpp:394] fc3 <- htanh
I0802 14:37:10.478139  6271 net.cpp:356] fc3 -> fc3
I0802 14:37:10.478217  6271 net.cpp:96] Setting up fc3
I0802 14:37:10.478416  6271 net.cpp:103] Top shape: 1 17 1 1 (17)
I0802 14:37:10.478468  6271 net.cpp:172] fc3 does not need backward computation.
I0802 14:37:10.478534  6271 net.cpp:172] htanh does not need backward computation.
I0802 14:37:10.478585  6271 net.cpp:172] fc1 does not need backward computation.
I0802 14:37:10.478653  6271 net.cpp:208] This network produces output fc3
I0802 14:37:10.478739  6271 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:37:10.478811  6271 net.cpp:219] Network initialization done.
I0802 14:37:10.478857  6271 net.cpp:220] Memory required for data: 2468
I0802 14:37:10.501251  6271 tonic.cpp:31] Reshaping input to dims: 28 375 1 1
I0802 14:37:10.501703  6271 tonic.cpp:37] Reshaping output to dims: 28 17 1 1
Command terminated by signal 9
93.17user 10.08system 3:02.14elapsed 56%CPU (0avgtext+0avgdata 15437936maxresident)k
182728inputs+0outputs (64major+1214274minor)pagefaults 0swaps
---------------------------------------------------------
colocating with libquantum
nlp-ner
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-ner
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:40:24.077975 19433 net.cpp:39] Initializing net from parameters: 
name: "ner"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 17
  }
}
input: "data"
input_dim: 1
input_dim: 375
input_dim: 1
input_dim: 1
I0802 14:40:24.083253 19433 net.cpp:358] Input 0 -> data
I0802 14:40:24.083721 19433 net.cpp:67] Creating Layer fc1
I0802 14:40:24.083803 19433 net.cpp:394] fc1 <- data
I0802 14:40:24.083863 19433 net.cpp:356] fc1 -> fc1
I0802 14:40:24.084008 19433 net.cpp:96] Setting up fc1
I0802 14:40:24.088121 19433 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:40:24.089723 19433 net.cpp:67] Creating Layer htanh
I0802 14:40:24.089833 19433 net.cpp:394] htanh <- fc1
I0802 14:40:24.089896 19433 net.cpp:356] htanh -> htanh
I0802 14:40:24.090078 19433 net.cpp:96] Setting up htanh
I0802 14:40:24.092294 19433 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:40:24.092710 19433 net.cpp:67] Creating Layer fc3
I0802 14:40:24.092777 19433 net.cpp:394] fc3 <- htanh
I0802 14:40:24.092943 19433 net.cpp:356] fc3 -> fc3
I0802 14:40:24.093062 19433 net.cpp:96] Setting up fc3
I0802 14:40:24.093188 19433 net.cpp:103] Top shape: 1 17 1 1 (17)
I0802 14:40:24.093276 19433 net.cpp:172] fc3 does not need backward computation.
I0802 14:40:24.093338 19433 net.cpp:172] htanh does not need backward computation.
I0802 14:40:24.093379 19433 net.cpp:172] fc1 does not need backward computation.
I0802 14:40:24.093439 19433 net.cpp:208] This network produces output fc3
I0802 14:40:24.093528 19433 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:40:24.093585 19433 net.cpp:219] Network initialization done.
I0802 14:40:24.093636 19433 net.cpp:220] Memory required for data: 2468
I0802 14:40:24.124902 19433 tonic.cpp:31] Reshaping input to dims: 28 375 1 1
I0802 14:40:24.125500 19433 tonic.cpp:37] Reshaping output to dims: 28 17 1 1
---------------------------------------------------------
colocating with libquantum
nlp-ner
Command terminated by signal 9
92.67user 9.71system 2:58.93elapsed 57%CPU (0avgtext+0avgdata 15462976maxresident)k
183008inputs+0outputs (65major+1215801minor)pagefaults 0swaps
********* Setting up benchmark nlp
********* Running benchmark as ./tonic-nlp-ner
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0802 14:43:32.666156 32110 net.cpp:39] Initializing net from parameters: 
name: "ner"
layers {
  bottom: "data"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 300
  }
}
layers {
  bottom: "fc1"
  top: "htanh"
  name: "htanh"
  type: HTANH
}
layers {
  bottom: "htanh"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  inner_product_param {
    num_output: 17
  }
}
input: "data"
input_dim: 1
input_dim: 375
input_dim: 1
input_dim: 1
I0802 14:43:32.669314 32110 net.cpp:358] Input 0 -> data
I0802 14:43:32.671221 32110 net.cpp:67] Creating Layer fc1
I0802 14:43:32.671546 32110 net.cpp:394] fc1 <- data
I0802 14:43:32.671852 32110 net.cpp:356] fc1 -> fc1
I0802 14:43:32.672001 32110 net.cpp:96] Setting up fc1
I0802 14:43:32.674592 32110 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:43:32.676602 32110 net.cpp:67] Creating Layer htanh
I0802 14:43:32.676741 32110 net.cpp:394] htanh <- fc1
I0802 14:43:32.676801 32110 net.cpp:356] htanh -> htanh
I0802 14:43:32.676908 32110 net.cpp:96] Setting up htanh
I0802 14:43:32.676991 32110 net.cpp:103] Top shape: 1 300 1 1 (300)
I0802 14:43:32.677054 32110 net.cpp:67] Creating Layer fc3
I0802 14:43:32.677094 32110 net.cpp:394] fc3 <- htanh
I0802 14:43:32.677156 32110 net.cpp:356] fc3 -> fc3
I0802 14:43:32.677214 32110 net.cpp:96] Setting up fc3
I0802 14:43:32.677379 32110 net.cpp:103] Top shape: 1 17 1 1 (17)
I0802 14:43:32.677439 32110 net.cpp:172] fc3 does not need backward computation.
I0802 14:43:32.677510 32110 net.cpp:172] htanh does not need backward computation.
I0802 14:43:32.677562 32110 net.cpp:172] fc1 does not need backward computation.
I0802 14:43:32.677603 32110 net.cpp:208] This network produces output fc3
I0802 14:43:32.677698 32110 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0802 14:43:32.677762 32110 net.cpp:219] Network initialization done.
I0802 14:43:32.677801 32110 net.cpp:220] Memory required for data: 2468
I0802 14:43:32.692200 32110 tonic.cpp:31] Reshaping input to dims: 28 375 1 1
I0802 14:43:32.692843 32110 tonic.cpp:37] Reshaping output to dims: 28 17 1 1
Command terminated by signal 9
91.82user 10.38system 2:59.80elapsed 56%CPU (0avgtext+0avgdata 15416848maxresident)k
201800inputs+0outputs (141major+1213042minor)pagefaults 0swaps
---------------------------------------------------------
